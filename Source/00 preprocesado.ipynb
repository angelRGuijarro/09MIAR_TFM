{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESADO\n",
    "\n",
    "En este fichero realizo las operaciones iniciales para \n",
    "- Conciliar los archivos de 'fichas' (xml) y 'escrituras' (pdf) que tienen la misma raíz en su nombre de archivo y copiarlos juntos en un directorio común, cambiándoles el nombre a un contador numérico de cinco caracteres.\n",
    "- Para cada par de ficheros: Buscar dentro en el texto extraído de la escritura los datos de la ficha. Si se encuentran, crear un archivo de texto (con el mismo contador por nombre) con el texto de la página que contiene estos datos. En este caso, la página será la portada de la escritura, pues es donde vienen los datos de registro.\n",
    "- Se genera también un archivo de texto con sufijo _norm.txt con el texto nomralizado. Sin espacios ni guiones repetidos, sin saltos de página y algunas correcciones detectadas en los textos extraídos.\n",
    "- A continuación se buscan en el texto las ocurrencias de los datos que vamos a extraer de las escrituras y que para el entrenamiento de este modelo podemos extraer de las fichas, se genera una estructura (dataset) con esta información.\n",
    "\n",
    "ANONIMIZACIÓN\n",
    "- Se extraen de las fichas los nombres, apellidos, razón social de empresas y nombres de vías para anonimizarlos en los textos de las escrituras, se generan con ellos una serie de diccionarios.\n",
    "- Se definen las funciones de anonimización, para los datos contenidos en las fichas y para el resto del texto (contexto). De manera que los datos de las fichas pueden ser referenciados en el texto aunque haya varias ocurrencias. En los datos de contexto se anonimizan todos los números de forma aleatoria y las entidades de diccionario se sustituyen por otras del mismo tipo.\n",
    "\n",
    "GENERACIÓN DEL DATASET\n",
    "- Se genera el dataset definitivo con los datos anonimizados.\n",
    "- Se prueba y se generan tres ficheros, que serán los utilizados durante el entrenemiento, la validación y el testeo de los modelos de QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import shutil\n",
    "from num2words import num2words\n",
    "import unicodedata\n",
    "from globals import RANDOM_APELLIDOS, RANDOM_NOMBRES, RANDOM_EMPRESAS, RANDOM_VIAS, DATA_DIR,PDF_FICHAS_DIR, ESC_DIR, FICH_DIR\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from enum import Enum, StrEnum, auto\n",
    "import datetime, time\n",
    "import ahocorasick # Librería para búsqueda de diccionarios\n",
    "import traceback\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 24 # Número de procesos utilizados en paralelización. Un número excesivo provoca saturación del sistema y ralentización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conciliar archivos de escrituras y fichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dejamos en una misma carpeta pdfs y fichas que empiecen por el mismo prefijo\n",
    "def copiar_archivos_coincidentes(ruta_pdf, ruta_fichas, ruta_destino):\n",
    "    \"\"\"Copia los archivos pdf y xml que tengan el mismo prefijo en su nombre de archivo a una sola carpeta.\\n\n",
    "    Los archivos de escritura acaban en '-copia.pdf' y los archivos de ficha acaban en '-ficha.xml'.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que el directorio de destino existe\n",
    "    if not os.path.exists(ruta_destino):\n",
    "        os.makedirs(ruta_destino)\n",
    "\n",
    "    # Obtener la lista de archivos PDF y fichas XML\n",
    "    archivos_pdf = [f for f in os.listdir(ruta_pdf) if f.endswith('-copia.pdf')]\n",
    "    archivos_fichas = [f for f in os.listdir(ruta_fichas) if f.endswith('-ficha.xml')]\n",
    "\n",
    "    # Inicializar el contador para los nombres secuenciales\n",
    "    contador = 0\n",
    "\n",
    "    for pdf in archivos_pdf:\n",
    "        # Obtener el nombre base del archivo PDF (sin '-copia.pdf')\n",
    "        nombre_base = pdf[:-10]\n",
    "\n",
    "        # Buscar la ficha correspondiente\n",
    "        ficha_correspondiente = nombre_base + '-ficha.xml'\n",
    "        if ficha_correspondiente in archivos_fichas:\n",
    "            # Construir las rutas completas de los archivos originales\n",
    "            ruta_origen_pdf = os.path.join(ruta_pdf, pdf)\n",
    "            ruta_origen_ficha = os.path.join(ruta_fichas, ficha_correspondiente)\n",
    "\n",
    "            # Construir las rutas de destino con nombres secuenciales\n",
    "            contador += 1\n",
    "            nombre_secuencial = f\"{contador:05d}\"\n",
    "            destino_pdf = os.path.join(ruta_destino, nombre_secuencial + '.pdf')\n",
    "            destino_ficha = os.path.join(ruta_destino, nombre_secuencial + '.xml')\n",
    "\n",
    "            # Copiar los archivos\n",
    "            shutil.copy2(ruta_origen_pdf, destino_pdf)\n",
    "            shutil.copy2(ruta_origen_ficha, destino_ficha)\n",
    "            \n",
    "    print(f\"{2*contador} Archivos copiados.({contador} escrituras)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones auxiliares de manejo de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quita_las_tildes(texto):\n",
    "    \"\"\"Normaliza el texto a la forma 'NFD' y elimina los caracteres de acento (dejando las eñes)\"\"\"\n",
    "    texto_sin_tildes = ''.join(\n",
    "            c if c in ['ñ', 'Ñ'] else ''.join(char for char in unicodedata.normalize('NFD', c) if unicodedata.category(char) != 'Mn')\n",
    "            for c in texto\n",
    "    )        \n",
    "    return texto_sin_tildes\n",
    "\n",
    "#se ha detectado que puede haber un espacio en el mes \"j ulio\", \"mar zo\" ... o en el año \"veinti dos\"\n",
    "def construir_regex_con_espacios(palabras, ignora_tildes=True):\n",
    "    # Construir una expresión regular que permite espacios opcionales entre cada letra\n",
    "    # Si ignora_tildes, además se incluye para cada vocal la posibilidad de tener o no tener tildes\n",
    "    if ignora_tildes:\n",
    "        vocales_tildes = {'a':'[aá]', 'e':'[eé]', 'i': '[ií]', 'o': '[oó]', 'u': '[uú]', \n",
    "                        'A':'[AÁ]', 'E':'[EÉ]', 'I':'[IÍ]', 'O':'[OÓ]', 'U':'[UÚ]'} \n",
    "        #buscamos cualquier vocal en la regex_con_espacios y la cambiamos por su representación con o sin tilde\n",
    "        patron_vocales = \"[aeiouAEIOU]\"\n",
    "        palabras = quita_las_tildes(palabras)\n",
    "        \n",
    "    palabras_separadas = palabras.split(' ')\n",
    "    regex_con_espacios = r'[\\W\\n]*'.join([r'\\b'+ r'[\\W\\n]*'.join(palabra) +r'\\b' for palabra in palabras_separadas])\n",
    "    if ignora_tildes:\n",
    "        regex_con_espacios=  re.sub(patron_vocales,lambda x: vocales_tildes.get(x.group(0),x.group(0)), regex_con_espacios)\n",
    "        \n",
    "    return regex_con_espacios\n",
    "\n",
    "# En las fichas, los datos de fecha están en formato YYYY-MM-DD, mientras que en las escrituras se utiliza un formato textualizado como 'siete de marzo de dos mil veinte'. Separaremos el mes y el año para hacer la búsqueda en el documento\n",
    "def separar_datos_fecha_a_texto(fecha):\n",
    "    \"\"\"Recibe una fecha en formato 'YYYY-MM-DD' y devuelve en formato texto (dia, mes,año)\"\"\"\n",
    "    partes = fecha.split('-')\n",
    "    meses = ['enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio',\n",
    "             'julio', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre']\n",
    "    dia = num2words(partes[2])\n",
    "    mes = meses[int(partes[1]) - 1]\n",
    "    año = num2words(int(partes[0]), lang='es', to='year')\n",
    "    return dia, mes, año\n",
    "\n",
    "def convertir_fecha_a_texto_completo(fecha):\n",
    "    \"\"\"Textualiza una fecha en formato YYYY-MM-DD\"\"\"\n",
    "    partes = fecha.split('-')\n",
    "    meses = ['enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio',\n",
    "             'julio', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre']\n",
    "    dia = num2words(int(partes[2]), lang='es')\n",
    "    mes = meses[int(partes[1]) - 1]\n",
    "    año = num2words(int(partes[0]), lang='es', to='year')\n",
    "    return f\"{dia} de {mes} de {año}\"\n",
    "\n",
    "def construir_regex_fecha(fecha):\n",
    "    \"\"\"A partir de una fecha en formato YYYY-MM-DD, devuelve una expresión regular para encontrarla en formato textual\"\"\"\n",
    "    partes = fecha.split('-')\n",
    "    meses = ['enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio',\n",
    "             'julio', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre']\n",
    "    dia = num2words(int(partes[2]), lang='es')\n",
    "    mes = meses[int(partes[1]) - 1]\n",
    "    año = num2words(int(partes[0]), lang='es', to='year')\n",
    "    return construir_regex_con_espacios(dia) + r'[\\n\\s]+de[\\n\\s]+' + construir_regex_con_espacios(mes) + r'[\\n\\s]+de((l|\\s+el)\\s+a(n|ñ)o)?' + construir_regex_con_espacios(año)\n",
    "\n",
    "def construir_regex_apellidos_notario(apellidos):\n",
    "    return r'\\s*(de\\s+|y\\s+)?'.join(list(map(construir_regex_con_espacios,apellidos.split(' '))))\n",
    "\n",
    "def construir_regex_notario(nombre, apellidos):\n",
    "    \"\"\"Dado que se ha encontrado que a veces los notarios en la escritura no indican su nombre completo (nombres compuestos) en la expresión regular se tratará de encontrar éste nombre\"\"\"\n",
    "    return construir_regex_con_espacios(nombre) + r'(\\s*[\\w\\-]*\\s*){0,3}' + construir_regex_apellidos_notario(apellidos)\n",
    "\n",
    "def reemplazo_segun_caso(match, txt_alternativo):\n",
    "    \"\"\"Función de reemplazo de cadenas, tratando de respetar el formato de mayúsculas y minúsculas.\\n\n",
    "    Si match y txt_alternativo no tienen el mismo número de palabras se tendrá en cuenta el caso en el número de palabras de la oración más corta.\"\"\"\n",
    "    txt_original = match.group()\n",
    "    if txt_original=='':\n",
    "        return ''\n",
    "    palabras_original = txt_original.split(' ')\n",
    "    palabras_alternativo = txt_alternativo.split(' ')\n",
    "    palabras_final = []\n",
    "    for original, alternativo in zip(palabras_original,palabras_alternativo):\n",
    "        if original.islower():\n",
    "            palabras_final.append(alternativo.lower())\n",
    "        elif original.isupper():\n",
    "            palabras_final.append(alternativo.upper())\n",
    "        elif original.istitle():\n",
    "           palabras_final.append(alternativo.title())\n",
    "        else:\n",
    "            palabras_final.append(alternativo)\n",
    "    # Unimos el resto de palabras del texto alternativo\n",
    "    palabras_final = palabras_final + palabras_alternativo[len(palabras_original):]\n",
    "    return ' '.join(palabras_final)\n",
    "\n",
    "def re_sub_case(patron_a_buscar, txt_alternativo, txt_original, flags = re.IGNORECASE):\n",
    "    \"\"\"Función de reemplazo de cadenas, tratando de respetar el formato de mayúsculas y minúsculas\"\"\"\n",
    "    patron_re = re.compile(patron_a_buscar, flags=flags)\n",
    "    return patron_re.sub(lambda x: reemplazo_segun_caso(x,txt_alternativo), txt_original)\n",
    "\n",
    "def normalizar_texto(texto,conservar_tildes=False):\n",
    "    \"\"\"Procesa el texto para devolver una versión\\nSin tildes (opcional)\\nSin saltos de página\\nSin espacios en blanco repetidos\\n\n",
    "        Sin guiones partiendo ni separando palabras\n",
    "    \"\"\"    \n",
    "    texto_inicial = texto if conservar_tildes else quita_las_tildes(texto)\n",
    "    # elimina saltos de página y mayúsculas\n",
    "    texto_sin_saltos = re.sub(r'\\n+', ' ', texto_inicial)\n",
    "\n",
    "    # Buscamos signos de igual repetidos (los usan para separar partes del texto, pero no dan información)\n",
    "    texto_sin_iguales = re.sub(r'\\s*=+\\s*', ' ', texto_sin_saltos)\n",
    "\n",
    "    # Esto evita que se quite la separación entre palabras completas con un guión en medio\n",
    "    texto_sin_guiones_sueltos = re.sub(r'\\s*-+\\s*', ' ', texto_sin_iguales)\n",
    "\n",
    "    # Elimina espacios repetidos\n",
    "    texto_sin_espacios_repetidos = re.sub(r'\\s+', ' ', texto_sin_guiones_sueltos)\n",
    "\n",
    "    # Elimina espacios antes y después de guiones\n",
    "    texto_sin_espacios_delante_guion = re.sub(r'(?<=\\w) -', '-', texto_sin_espacios_repetidos)\n",
    "    texto_sin_espacios_detras_guion = re.sub(r'- (?=\\w)', '-', texto_sin_espacios_delante_guion)\n",
    "\n",
    "    # Elimina guiones entre palabras\n",
    "    texto_sin_guiones = re.sub(r'(?<=\\w)-(?=\\w)', '', texto_sin_espacios_detras_guion)\n",
    "\n",
    "    return texto_sin_guiones\n",
    "\n",
    "def subsanar_errores_de_lectura(texto)->str:\n",
    "    \"\"\"Se han detectado errores comunes al leer el texto desde un fichero pdf. Esta función trata de arreglar esos errores\"\"\"\n",
    "    def corregir_meses_años(texto):\n",
    "        # Corregir meses\n",
    "        meses = [\"enero\", \"febrero\", \"marzo\", \"abril\", \"mayo\", \"junio\", \"julio\", \"agosto\", \"septiembre\", \"octubre\", \"noviembre\", \"diciembre\"]\n",
    "        for mes in meses:\n",
    "            regex = construir_regex_con_espacios(mes)\n",
    "            texto = re_sub_case(regex, mes, texto)\n",
    "\n",
    "        # Corregir años entre veintiuno y veintinueve\n",
    "        year = [\"uno\", \"dos\", \"trés\", \"cuatro\", \"cinco\", \"séis\", \"siete\", \"ocho\", \"nueve\"]\n",
    "        for num in year:\n",
    "            regex = construir_regex_con_espacios(f\"veinti{num}\")\n",
    "            texto = re_sub_case(regex, f\"veinti{num}\", texto)\n",
    "\n",
    "        return texto\n",
    "\n",
    "    #en este caso se ha detectado que a veces, en la fecha se escribe \"mi l\" en lugar de \"mil\"\n",
    "    texto_corregido = re_sub_case(construir_regex_con_espacios('mil'), 'mil', texto)\n",
    "    texto_corregido = corregir_meses_años(texto_corregido)    \n",
    "\n",
    "    return texto_corregido\n",
    "\n",
    "def extraer_textos_del_pdf(ruta_pdf) -> [str]:\n",
    "    \"\"\"Extrae el texto de todas las páginas.\\nDevuelve una lista con el texto de cada página.\"\"\"\n",
    "    textos = []\n",
    "    with open(ruta_pdf, 'rb') as archivo:\n",
    "        lector = PyPDF2.PdfReader(archivo)\n",
    "        textos = [pagina.extract_text() for pagina in lector.pages]\n",
    "        \n",
    "    return textos            \n",
    "\n",
    "def extraer_de_pdf_pagina_con_regex(ruta_pdf, lista_regex):\n",
    "    \"\"\"Devuelve el texto de aquella página del documento pdf donde se encuentren todas las coincidencias con la lista de expresiones regulares 'lista_regex'\"\"\"\n",
    "    with open(ruta_pdf, 'rb') as archivo:\n",
    "        lector = PyPDF2.PdfReader(archivo)\n",
    "        for i in range(len(lector.pages)):\n",
    "            pagina = lector.pages[i]\n",
    "            texto_pagina = pagina.extract_text()\n",
    "            texto_pagina_normalizado_y_corregido = subsanar_errores_de_lectura(normalizar_texto(texto_pagina))\n",
    "            if all(re.search(regex, texto_pagina_normalizado_y_corregido,re.IGNORECASE) is not None \n",
    "                   for regex in lista_regex):\n",
    "                return texto_pagina\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extraer_de_pdf_pagina_con_textos(ruta_pdf, lista_textos):\n",
    "    \"\"\"Abre el fichero pdf encontrado en ruta_pdf y busca los textos de la lista_textos.\\n\n",
    "    Devuelve el texto original de la página que contiene todos los textos buscados.\\n\n",
    "    Si no encuentra todos los textos de lista_textos entonces devuelve None\n",
    "    \"\"\"\n",
    "    texto_buscado = [normalizar_texto(texto) for texto in lista_textos]\n",
    "\n",
    "    with open(ruta_pdf, 'rb') as archivo:\n",
    "        lector = PyPDF2.PdfReader(archivo)\n",
    "        for i in range(len(lector.pages)):\n",
    "            pagina = lector.pages[i]\n",
    "            texto_pagina = pagina.extract_text()\n",
    "            texto_pagina_normalizado_y_corregido = subsanar_errores_de_lectura(normalizar_texto(texto_pagina))\n",
    "            if all(re.search(construir_regex_con_espacios(texto), texto_pagina_normalizado_y_corregido, re.IGNORECASE) is not None for texto in texto_buscado):\n",
    "                return texto_pagina\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer el texto de la portada que contiene los datos de registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_namespace(root):\n",
    "    return {'ns': re.match(r'\\{(.*)\\}',root.tag).group(1)}\n",
    "    \n",
    "def extraer_txt_portada_escritura(ruta_documento_xml):\n",
    "    \"\"\"Lee las páginas del PDF asociado hasta que aparezca una página donde esté el nombre del notario y la fecha (esta es la carátula).\\n\n",
    "        Crea un .txt con el texto de sólo esa página, con el mismo nombre base del documento xml.\"\"\"\n",
    "    if os.path.splitext(ruta_documento_xml)[1]=='.xml':\n",
    "        ruta_carpeta = os.path.dirname(ruta_documento_xml)\n",
    "        archivo = os.path.basename(ruta_documento_xml)\n",
    "\n",
    "        nombre_txt = archivo.replace('.xml', '.txt')\n",
    "        ruta_txt = os.path.join(ruta_carpeta, nombre_txt)\n",
    "        nombre_txt_normalizado = archivo.replace('.xml', '_norm.txt')\n",
    "        ruta_txt_normalizado = os.path.join(ruta_carpeta, nombre_txt_normalizado)\n",
    "        # Comprobar si el archivo .txt ya existe\n",
    "        if os.path.exists(ruta_txt):\n",
    "            return\n",
    "        \n",
    "        tree = ET.parse(ruta_documento_xml)\n",
    "        root = tree.getroot()\n",
    "        namespace = get_xml_namespace(root)\n",
    "\n",
    "        apellidos_notario = root.find('.//ns:CABECERA/ns:NOT/ns:NOTARIO/ns:APELLIDOS', namespace).text\n",
    "        fecha_autorizacion = root.find('.//ns:DATOS/ns:DOCUMENTO/ns:IDE_DOC/ns:FECHA_AUTORIZACION', namespace).text\n",
    "\n",
    "        dia_texto, mes_texto, año_texto = separar_datos_fecha_a_texto(fecha_autorizacion)\n",
    "\n",
    "        # Nombre del archivo PDF asociado\n",
    "        nombre_pdf = archivo.replace('.xml', '.pdf')                    \n",
    "        ruta_pdf = os.path.join(ruta_carpeta, nombre_pdf)\n",
    "\n",
    "        # Buscar texto en PDF\n",
    "        #texto_buscado = [nombre_notario, apellidos_notario, mes_texto, año_texto]        \n",
    "        #Voy a quitar de la búsqueda el nombre del notario, porque a veces los notarios no ponen su nombre completo\n",
    "        #Tampoco uso el formato de fecha completa, porque a veces ponen 5 de enero de 2020 y otras veces 5 de enero del año 2020, con mes y año tenemos bastante\n",
    "        texto_buscado = [apellidos_notario, mes_texto, año_texto]\n",
    "        # regex_buscados = [regex_apellidos_notario, regex_fecha]\n",
    "        texto_encontrado = extraer_de_pdf_pagina_con_textos(ruta_pdf, texto_buscado)\n",
    "        # texto_encontrado = extraer_de_pdf_pagina_con_regex(ruta_pdf, regex_buscados)\n",
    "\n",
    "        if texto_encontrado:\n",
    "            with open(ruta_txt, 'w', encoding='utf-8') as archivo_txt:\n",
    "                archivo_txt.write(texto_encontrado)\n",
    "            #Guardo también una copia \"normalizada y subsanada\" para ver si es más efectiva la extracción con los datos depurados o aprende más con los datos crudos\n",
    "            with open(ruta_txt_normalizado, 'w', encoding='utf-8') as archivo_norm_txt:\n",
    "                archivo_norm_txt.write(normalizar_texto(subsanar_errores_de_lectura(texto_encontrado),conservar_tildes=True))\n",
    "        else:\n",
    "            # La copia _no_encontrado está pensada para revisar el texto y ver por qué no se han encontrado los datos\n",
    "            ruta_no_encontrado = os.path.join(ruta_carpeta, \"_no_encontrado_\"+nombre_txt)\n",
    "            with open(ruta_no_encontrado,'w', encoding='utf-8') as archivo_no_encontrado:\n",
    "                archivo_no_encontrado.write('\\n'.join(extraer_textos_del_pdf(ruta_pdf)))\n",
    "\n",
    "# Automatización para hacerlo a toda la carpeta\n",
    "def extraer_txt_portada_escrituras(ruta_carpeta):\n",
    "    \"\"\"Busca los datos del xml en el pdf y si los encuentra crea un txt con el texto de esa página\"\"\"    \n",
    "    # Ejecución en paralelo para ahorrar tiempo\n",
    "    archivos_xml = [os.path.join(ruta_carpeta,archivo) for archivo in os.listdir(ruta_carpeta) if archivo.endswith('.xml')]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ejecutor:\n",
    "        ejecutor.map(extraer_txt_portada_escritura,archivos_xml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función auxliar para buscar los .txt que no se han procesado\n",
    "def ficheros_faltantes(directorio):\n",
    "    # Obtener la lista de archivos en el directorio\n",
    "    archivos = os.listdir(directorio)\n",
    "\n",
    "    # Filtrar archivos .txt que siguen el patrón de nombres\n",
    "    # archivos_txt = [archivo for archivo in archivos if archivo.endswith('.txt') and archivo[:-4].isdigit()]\n",
    "    archivos_txt = [archivo for archivo in os.listdir(directorio) \n",
    "                    for nombre, extension in (os.path.splitext(archivo),) if nombre.isdigit() and extension=='.txt']\n",
    "    # Extraer los números de los archivos y convertirlos a enteros\n",
    "    numeros_archivos = [int(archivo.split('.')[0]) for archivo in archivos_txt]\n",
    "\n",
    "    # Encontrar los números faltantes\n",
    "    if numeros_archivos:\n",
    "        max_numero = max(numeros_archivos)\n",
    "        numeros_faltantes = set(range(1, max_numero + 1)) - set(numeros_archivos)\n",
    "        return sorted(list(numeros_faltantes))\n",
    "    else:\n",
    "        return \"No hay archivos .txt que sigan el patrón en el directorio\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PREPROCESADO DE LOS FICHEROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conciliar los archivos de 'fichas' (xml) y 'escrituras' (pdf) que tienen la misma raíz en su nombre de archivo y copiarlos juntos a un directorio común.\n",
    "copiar_archivos_coincidentes(ESC_DIR,FICH_DIR,PDF_FICHAS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar dentro en el texto extraído de la escritura los datos de la ficha, si se encuentran, crear un archivo de texto (autonumerado) con el texto de la página que contiene estos datos. En este caso, la página será la portada de la escritura, pues es donde vienen los datos de registro. Se crea un segundo archivo normalizado.\n",
    "extraer_txt_portada_escrituras(PDF_FICHAS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBA\n",
    "# # Compruebo cuántas escrituras se han quedado sin archivo de texto, para comprobar cuál puede ser el motivo y mejorar la función de extracción.\n",
    "# faltantes = ficheros_faltantes(os.path.join(PDF_FICHAS_DIR))\n",
    "# print(f\"{len(faltantes)} faltantes\\n\",faltantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, busco el tipo de documento de cada escritura. En las fichas no aparece especificado, así que hay que buscarlo de forma textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino estos tipos fuera por ser constantes y no tener que volver a definirlos en cada llamada\n",
    "TIPOS_ESCRITURA = ['compraventa', 'herencia', 'herencias', 'dación', 'daciones', 'donación', 'donaciones', 'cesión',\n",
    "                       'disolución', 'extinción', 'legado', 'legados', 'liquidación', 'cesión', 'permuta']\n",
    "RE_TIPOS_ESCRITURA = '('+ '|'.join(list(map(construir_regex_con_espacios,TIPOS_ESCRITURA))) + ')'\n",
    "# Como algunos tipos de documento aparecen en singular y plurar, los juntaré en uno solo\n",
    "TIPOS_BASE = {\n",
    "    'HERENCIAS': 'HERENCIA', \n",
    "    'DONACIONES': 'DONACION', \n",
    "    'LEGADOS': 'LEGADO'\n",
    "}\n",
    "\n",
    "#Función para buscar el tipo de documento de un archivo con el texto de la escritura\n",
    "def extraer_datos_tipo_documento(ruta_archivo_txt=None, contenido_archivo=None, incluye_texto=False):\n",
    "    \"\"\"Lee los datos de ruta_archivo_txt y busca el tipo de documento. Si el archivo ya ha sido abierto y se dispone del texto, puede pasarse en contenido_archivo para mejorar la eficiencia y evitar lecturas de disco.\n",
    "    \"\"\"\n",
    "    # Procesar archivo TXT para descripción\n",
    "    if contenido_archivo:\n",
    "        contenido = contenido_archivo\n",
    "        numero_archivo = -1\n",
    "    elif ruta_archivo_txt:\n",
    "        numero_archivo, extension = os.path.splitext(os.path.basename(ruta_archivo_txt))     \n",
    "        assert extension =='.txt', \"La extracción de los datos de tipo de documento ha de hacerse en archivos de texto\"\n",
    "        with open(ruta_archivo_txt, 'r', encoding='utf-8') as file:\n",
    "            contenido = file.read()        \n",
    "    else:\n",
    "        raise ValueError(\"Debe aportarse una ruta de archivo o el contenido del mismo.\")\n",
    "    \n",
    "    # Buscar con expresión regular los tipos de escritura\n",
    "    descripcion = re.search(RE_TIPOS_ESCRITURA + r'.*?(?=[\\-.,»>_])', contenido, re.IGNORECASE | re.DOTALL)\n",
    "    if descripcion:\n",
    "        texto_encontrado = descripcion.group(1)\n",
    "        posicion_texto_encontrado = descripcion.start(1)\n",
    "        #Si hemos conseguido encontrar el tipo de escritura, generamos el tipo de documento normalizado\n",
    "        match = re.search(RE_TIPOS_ESCRITURA , texto_encontrado, re.IGNORECASE)        \n",
    "        tipo_documento = normalizar_texto(match.group().strip().upper()).replace(\" \", \"\")        \n",
    "    else:\n",
    "        texto_encontrado = \"\"\n",
    "        tipo_documento = \"\"                \n",
    "        posicion_texto_encontrado = -1\n",
    "\n",
    "    tipo_documento = TIPOS_BASE.get(tipo_documento,tipo_documento)\n",
    "    \n",
    "    resultado = {\n",
    "        'numero': numero_archivo,\n",
    "        'tipo_documento': tipo_documento,\n",
    "        'texto_encontrado' : texto_encontrado,\n",
    "        'posicion_texto_encontrado' : posicion_texto_encontrado    \n",
    "    }           \n",
    "    if incluye_texto:\n",
    "        resultado['texto'] = contenido\n",
    "\n",
    "    return resultado\n",
    "\n",
    "#Necesito comprobar todos los tipos de documento que hay, con su número asociado en la ficha xml\n",
    "def extraer_datos_tipo_documentos(directorio):\n",
    "    \"\"\"Genera una lista con información sobre los tipos de documento encontrados en el directorio a partir de los ficheros .xml y sus correspondientes .txt.\\n\n",
    "    'numero' es el número del archivo (en cadena de cinco caracteres)\\n\n",
    "    'tipo_documento' es el tipo de documento normalizado\\n\n",
    "    'texto_encontrado' es cómo se lee en el archivo, puede tener defectos de formato\\n\n",
    "    'posicion_texto_encontrado' es la posición dentro del texto donde se ha encontrado el tipo de documento.    \n",
    "    'texto' se incluye el texto completo para funciones de depuración\n",
    "    \"\"\"\n",
    "    datos = []\n",
    "    archivos_xml = [archivo for archivo in os.listdir(directorio) if archivo.endswith('.xml')]    \n",
    "    \n",
    "    for archivo in sorted(archivos_xml):        \n",
    "        ruta_archivo_txt = os.path.join(directorio, archivo.replace('.xml','.txt'))\n",
    "        # #Si no tenemos txt es que no hemos podido encontrar todos los campos en este documento       \n",
    "        if not os.path.exists(ruta_archivo_txt):\n",
    "            continue            \n",
    "        datos.append(extraer_datos_tipo_documento(ruta_archivo_txt,incluye_texto=True))\n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBA\n",
    "# # Dejamos los datos del tipo de documento guardados en esta variable\n",
    "# datos_tipo_documento = extraer_datos_tipo_documentos(PDF_FICHAS_DIR)\n",
    "\n",
    "# # Muestro una pequeña estadística para ver la distribución de los tipos de documento.\n",
    "# tipos = {}\n",
    "# for d in datos_tipo_documento:\n",
    "#     tipo = d['tipo_documento']\n",
    "#     tipos[tipo] = tipos.get(tipo,0) + 1 #guardamos el número de referencias encontradas    \n",
    "\n",
    "# for clave,valor in sorted(tipos.items(), key=lambda item: item[1], reverse=True):\n",
    "#     print(valor, clave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBA\n",
    "# # Dejamos los datos del tipo de documento guardados en esta variable\n",
    "# datos_tipo_documento = extraer_datos_tipo_documentos(PDF_FICHAS_DIR)\n",
    "\n",
    "# # Muestro una pequeña estadística para ver la distribución de los tipos de documento.\n",
    "# tipos = {}\n",
    "# for d in datos_tipo_documento:\n",
    "#     # tipo = d['tipo_documento']\n",
    "#     tipo = d['texto_encontrado']\n",
    "#     tipos[tipo] = tipos.get(tipo,0) + 1 #guardamos el número de referencias encontradas    \n",
    "\n",
    "# for clave,valor in sorted(tipos.items(), key=lambda item: item[1], reverse=True):\n",
    "#     print(valor, clave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBA\n",
    "#Comprobación para ver los tipos no identificados\n",
    "# with open(os.path.join(PDF_FICHAS_DIR,\"_sin_tipo_identificado.txt\"),'w',encoding='utf-8') as salida:    \n",
    "#     for d in [d for d in datos_tipo_documento if d['tipo_documento']==\"\"]:\n",
    "#         # print(d['texto'])\n",
    "#         salida.write(d['texto'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones auxiliares para la preparación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_en_texto(re_busqueda, texto, grupo = 0):\n",
    "    \"\"\"Función que busca la espresión regular 're_busqueda' en 'texto' y devuelve la posición de inicio y el texto encontrado en una lista de diccionarios [{'answer_start':int, 'text':str}] para el grupo indicado y cada una de las ocurrencias encontradas. De este modo, puede elegirse si de toda la expresión buscada sólo quiere localizarse una parte del texto como respuesta.\\n\n",
    "    Si no encuentra el texto devuelve [{'answer_start':-1, 'text':\"\"}]\"\"\"    \n",
    "    coincidencias = []    \n",
    "    for match in re.finditer(re_busqueda, texto,flags=re.IGNORECASE):\n",
    "        coincidencias.append({\"answer_start\": match.start(grupo), \"text\": match.group(grupo)})\n",
    "    \n",
    "    if len(coincidencias)==0:\n",
    "        coincidencias.append({\"answer_start\": -1, \"text\": \"\"})\n",
    "\n",
    "    return coincidencias\n",
    "\n",
    "class Tipo_Contenido(StrEnum):\n",
    "    \"\"\"Enumerado para trabajar con los tipos texto para diferenciar las preguntas del contexto\"\"\"\n",
    "    # Tipos de preguntas #\n",
    "    PROTOCOLO = auto()\n",
    "    FECHA = auto()\n",
    "    NOTARIO = auto()\n",
    "    TIPO_DOCUMENTO = 'tipo'\n",
    "    # Tipos que no están codificados como preguntas #\n",
    "    CONTEXT = auto()    \n",
    "    \n",
    "    @staticmethod\n",
    "    def tipo(question:str)->StrEnum|None:\n",
    "        \"\"\"Indica de qué tipo es esta pregunta\"\"\"\n",
    "        for tipo_c in Tipo_Contenido:\n",
    "            if tipo_c in [Tipo_Contenido.PROTOCOLO, Tipo_Contenido.FECHA, Tipo_Contenido.NOTARIO, Tipo_Contenido.TIPO_DOCUMENTO] \\\n",
    "                and tipo_c in question.lower():\n",
    "                return tipo_c            \n",
    "        # Si no es un tipo dentro de los esperados para preguntas, es None\n",
    "        return None\n",
    "    @staticmethod\n",
    "    def pregunta(tipo:StrEnum):\n",
    "        \"\"\"Devuelve la pregunta que debe hacerse para ser de este tipo, si el tipo no está pensado para ser preguntado, devuelve None\"\"\"\n",
    "        if tipo == Tipo_Contenido.PROTOCOLO:\n",
    "            return \"¿cuál es el número de protocolo?\"\n",
    "        elif tipo == Tipo_Contenido.NOTARIO:\n",
    "            return \"¿qué notario ha firmado el documento?\"\n",
    "        elif tipo == Tipo_Contenido.FECHA:\n",
    "            return \"¿en qué fecha se ha firmado el documento?\"\n",
    "        elif tipo == Tipo_Contenido.TIPO_DOCUMENTO:\n",
    "            return \"¿cuál es el tipo de documento?\"\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "def generar_datos_QA(ruta_archivo_xml, usar_txt_normalizado=False,xml_precargado=None, texto_precargado=None):    \n",
    "    \"\"\"Dado un archivo con los datos de la ficha (en xml) busca las coincidencias en el archivo de texto extraido de la escritura, y devuelve un objeto diccionario con los datos para el entrenamiento de QA\\n\n",
    "    El objeto devuelto tiene la siguiente estructura {'id_documento':str, 'context':str, 'qas':list({'question':str, 'answers':list({'answer_start':int, 'text':str})})}\\n\n",
    "    Si no existe fichero txt asociado al xml con el texto de la escritura, se devuelve None\"\"\"\n",
    "    ruta, nombre_archivo  = os.path.split(os.path.splitext(ruta_archivo_xml)[0]) # omito la extensión\n",
    "    try:\n",
    "        if texto_precargado is None:\n",
    "            extension = '_norm.txt' if usar_txt_normalizado else '.txt'\n",
    "            with open(os.path.join(ruta, nombre_archivo+extension), 'r', encoding='utf-8') as txt_file:\n",
    "                texto_pagina = txt_file.read()\n",
    "        else:    \n",
    "            texto_pagina = texto_precargado\n",
    "    except:\n",
    "        #Si no hay archivo de texto no podemos generar datos\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if xml_precargado is None:\n",
    "            arbol = ET.parse(ruta_archivo_xml)\n",
    "        else:\n",
    "            arbol = xml_precargado\n",
    "\n",
    "        raiz = arbol.getroot()        \n",
    "        namespace = get_xml_namespace(raiz)\n",
    "\n",
    "        numero_protocolo = num2words(int(raiz.find(\".//ns:DATOS/ns:DOCUMENTO/ns:IDE_DOC/ns:NUMERO_PROTOCOLO\", namespace).text),lang='es')\n",
    "        re_protocolo = construir_regex_con_espacios(numero_protocolo)\n",
    "        # Tomo sólo el primer nombre del notario, para tratar de pillar los que tienen nombre compuesto y sólo usan el primero\n",
    "        # en la expresión regular se permite la localización de más nombres entre el primer nombre y los apellidos\n",
    "        nombre_notario = raiz.find(\".//ns:CABECERA/ns:NOT/ns:NOTARIO/ns:NOMBRE\", namespace).text.split(' ')[0]\n",
    "        apellidos_notario =  raiz.find(\".//ns:CABECERA/ns:NOT/ns:NOTARIO/ns:APELLIDOS\", namespace).text\n",
    "        re_notario = construir_regex_notario(nombre_notario, apellidos_notario)\n",
    "        re_fecha_documento = construir_regex_fecha(raiz.find(\".//ns:DATOS/ns:DOCUMENTO/ns:IDE_DOC/ns:FECHA_AUTORIZACION\", namespace).text)          \n",
    "\n",
    "        preguntas = [\n",
    "                (Tipo_Contenido.pregunta(Tipo_Contenido.PROTOCOLO), re_protocolo),\n",
    "                (Tipo_Contenido.pregunta(Tipo_Contenido.NOTARIO), re_notario),\n",
    "                (Tipo_Contenido.pregunta(Tipo_Contenido.FECHA), re_fecha_documento)\n",
    "            ]\n",
    "        \n",
    "        escritura = {\n",
    "            'id_documento': nombre_archivo,\n",
    "            'context': texto_pagina,\n",
    "            'qas':[]\n",
    "        }\n",
    "        for pregunta, re_busqueda in preguntas:\n",
    "            escritura['qas'].append(\n",
    "                {\n",
    "                    'question': pregunta,\n",
    "                    'answers': buscar_en_texto(re_busqueda, texto_pagina)\n",
    "                }\n",
    "            )\n",
    "        #La pregunta del tipo de documento es algo más complicada\n",
    "        tipo_documento = extraer_datos_tipo_documento(contenido_archivo=texto_pagina)    \n",
    "        escritura['qas'].append(\n",
    "            {\n",
    "                'question': Tipo_Contenido.pregunta(Tipo_Contenido.TIPO_DOCUMENTO),\n",
    "                'answers': [{'answer_start': tipo_documento['posicion_texto_encontrado'], \n",
    "                            'text': tipo_documento['texto_encontrado']}]            \n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"EXCEPCIÓN en generar_datos_QA({ruta_archivo_xml})\\n\",e)\n",
    "        return None\n",
    "    \n",
    "    return escritura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PRUEBA\n",
    "# # Prueba de extracción de datos\n",
    "# datos = generar_datos_QA(os.path.join(PDF_FICHAS_DIR, \"03601.xml\"),True)\n",
    "# print(datos['context'],'\\n')\n",
    "# for qa in datos['qas']:\n",
    "#     print(qa['question'], qa['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_dataset_QA(directorio_origen, ruta_destino):\n",
    "    \"\"\"Genera un archivo .json con el dataset de los archivos de escritura.\\n\n",
    "    El fichero contendrá un objeto data con una lista de escrituras, cada escritura tiene los datos que produce la función 'gererar_datos_QA'\"\"\"\n",
    "    resultado = {'data':[]}\n",
    "    # Para iterar utilizamos las fichas (archivos xml)\n",
    "    lista_archivos = [os.path.join(directorio_origen, a) for a in os.listdir(directorio_origen) if a.endswith('.xml')]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ejecutor:    \n",
    "        # Ejecuciones guarda un objeto Future por cada archivo xml, con los resultados de su ejecución\n",
    "        ejecuciones = {ejecutor.submit(generar_datos_QA, archivo_xml):archivo_xml for archivo_xml in lista_archivos}\n",
    "        for ejecucion in concurrent.futures.as_completed(ejecuciones): # conforme se van completando van apareciendo            \n",
    "            escritura = ejecucion.result()\n",
    "            if escritura is not None:\n",
    "                resultado['data'].append({'escritura':escritura})\n",
    "    \n",
    "    with open(ruta_destino, 'w', encoding='utf-8') as outfile:    \n",
    "        json.dump(resultado, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PRUEBA\n",
    "# generar_dataset_QA(PDF_FICHAS_DIR,os.path.join(DATA_DIR,'Inicial','dataset_QA.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANONIMIZACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero una serie de archivos, con los datos de nombres, apellidos, nombres de empresa y vías a partir de los datos de las fichas. Éstos deben ser los datos sensibles que contienen todas las escrituras utlizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_datos_ficha(ruta_ficha_xml):\n",
    "    \"\"\"Función para sacar todos los datos críticos de una ficha. Devuelve un diccionario con {'tipo','entrada'}\"\"\"    \n",
    "    try:\n",
    "        arbol = ET.parse(ruta_ficha_xml)\n",
    "        raiz = arbol.getroot()\n",
    "        namespaces = {'ns': re.match(r'\\{(.*)\\}',raiz.tag).group(1)}\n",
    "        \n",
    "        datos = []\n",
    "        # Comprobamos primero si es una persona física (tiene campo NOM)\n",
    "        for per in  raiz.findall('.//ns:PER', namespaces):\n",
    "            nom = per.find('.//ns:NOM', namespaces)\n",
    "            # Buscar y guardar los apellidos\n",
    "            ape1 = per.find('.//ns:APE1_RAZ_SOC', namespaces)\n",
    "            ape2 = per.find('.//ns:APE2', namespaces)\n",
    "            # Si tiene nombre es persona, guardamos sus apellidos\n",
    "            if nom is not None and nom.text:\n",
    "                datos.append({\n",
    "                    'tipo': 'nombre',\n",
    "                    'entrada': nom.text.strip()\n",
    "                })             \n",
    "                if ape1 is not None and ape1.text :\n",
    "                    datos.append({\n",
    "                        'tipo': 'apellido',\n",
    "                        'entrada':ape1.text.strip()\n",
    "                    })\n",
    "                if ape2 is not None and ape2.text:\n",
    "                    datos.append({\n",
    "                        'tipo': 'apellido',\n",
    "                        'entrada':ape2.text.strip()\n",
    "                    })\n",
    "            # Si no, será empresa, sólo guardo su razón social\n",
    "            elif ape1 is not None and ape1.text: \n",
    "                datos.append({\n",
    "                    'tipo': 'empresa',\n",
    "                    'entrada':ape1.text.strip()\n",
    "                })\n",
    "        #Lo mismo, pero ahora para las direcciones\n",
    "        for via in raiz.findall('.//ns:VIA', namespaces):\n",
    "            datos.append({\n",
    "                'tipo': 'via',\n",
    "                'entrada': via.text.strip()\n",
    "            })  \n",
    "    except ET.ParseError:\n",
    "        print(f\"Error al parsear el archivo: {ruta_ficha_xml}\")\n",
    "    \n",
    "    return datos\n",
    "\n",
    "# Voy a probar a paralelizar la lectura de los ficheros para luego guardar todos los datos sin repetición en los ficheros de datos.\n",
    "def generar_ficheros_de_datos(ruta_ficheros_xml, fichero_nombres, fichero_apellidos, fichero_empresas, fichero_vias):\n",
    "    lista_archivos_xml = [os.path.join(ruta_ficheros_xml,archivo) for archivo in os.listdir(ruta_ficheros_xml) if archivo.endswith('.xml')]\n",
    "    \n",
    "    set_nombres = set()\n",
    "    set_apellidos = set()\n",
    "    set_empresas = set()\n",
    "    set_vias = set()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as ejecutor:\n",
    "        # Ejecuciones guarda un objeto Future por cada archivo xml, con los resultados de su ejecución\n",
    "        ejecuciones = {ejecutor.submit(extraer_datos_ficha, archivo_xml):archivo_xml for archivo_xml in lista_archivos_xml}\n",
    "        for ejecucion in concurrent.futures.as_completed(ejecuciones): # conforme se van completando van apareciendo\n",
    "            datos = ejecucion.result()\n",
    "            for d in datos:\n",
    "                # Si es una empresa o una calle guardamos la cadena entera, si no, por partes\n",
    "                if d['tipo']== 'nombre':                    \n",
    "                    set_nombres.add(d['entrada'])\n",
    "                elif d['tipo']== 'apellido':\n",
    "                    set_apellidos.add(d['entrada'])\n",
    "                elif d['tipo']== 'empresa':\n",
    "                    set_empresas.add(d['entrada'])                \n",
    "                elif d['tipo']== 'via':\n",
    "                    set_vias.add(d['entrada'])\n",
    "                else:\n",
    "                    print(f\"DEBUG: Se ha encontrado un tipo no manejado {d['tipo']}\")\n",
    "\n",
    "    with open(fichero_nombres,'w',encoding='utf-8') as f_nombres, \\\n",
    "        open(fichero_apellidos,'w',encoding='utf-8') as f_apellidos, \\\n",
    "        open(fichero_vias,'w',encoding='utf-8') as f_vias, \\\n",
    "        open(fichero_empresas,'w',encoding='utf-8') as f_empresas:        \n",
    "        def add_new_line(x): \n",
    "            return x+'\\n' \n",
    "        def guarda_fichero(conjunto_datos,fichero):            \n",
    "            lista_con_saltos_de_linea = list(map(add_new_line,list(conjunto_datos)))\n",
    "            lista_con_saltos_de_linea[-1] = lista_con_saltos_de_linea[-1].strip()\n",
    "            fichero.writelines(lista_con_saltos_de_linea)        \n",
    "\n",
    "        guarda_fichero(set_nombres,f_nombres)\n",
    "        guarda_fichero(set_apellidos,f_apellidos)\n",
    "        guarda_fichero(set_empresas,f_empresas)\n",
    "        guarda_fichero(set_vias,f_vias)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de los ficheros de datos (Diccionarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_ficheros_de_datos(PDF_FICHAS_DIR,RANDOM_NOMBRES, RANDOM_APELLIDOS,RANDOM_EMPRESAS,RANDOM_VIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino funciones para anonimizar los nombres y apellidos\n",
    "DATA_NOMBRES = []\n",
    "DATA_APELLIDOS = []\n",
    "DATA_EMPRESAS = []\n",
    "DATA_VIAS = []\n",
    "\n",
    "def cargar_aleatorios():\n",
    "    global DATA_NOMBRES\n",
    "    global DATA_APELLIDOS\n",
    "    global DATA_EMPRESAS\n",
    "    global DATA_VIAS\n",
    "\n",
    "    with open(RANDOM_NOMBRES, 'r', encoding='utf-8') as archivo:\n",
    "        DATA_NOMBRES = archivo.readlines()\n",
    "    with open(RANDOM_APELLIDOS, 'r', encoding='utf-8') as archivo:\n",
    "        DATA_APELLIDOS = archivo.readlines()\n",
    "    with open(RANDOM_EMPRESAS, 'r', encoding='utf-8') as archivo:\n",
    "        DATA_EMPRESAS = archivo.readlines()\n",
    "    with open(RANDOM_VIAS, 'r', encoding='utf-8') as archivo:\n",
    "        DATA_VIAS = archivo.readlines()\n",
    "\n",
    "cargar_aleatorios()\n",
    "\n",
    "def obtener_nombre_aleatorio():\n",
    "    return random.choice(DATA_NOMBRES).strip()\n",
    "\n",
    "def obtener_apellido_aleatorio():\n",
    "    return random.choice(DATA_APELLIDOS).strip()\n",
    "\n",
    "def obtener_empresa_aleatoria(empresa):    \n",
    "    aleatoria = empresa.split(' ')\n",
    "    for orden in range(len(aleatoria)):\n",
    "        busca = orden\n",
    "        while busca>=0:            \n",
    "            try:\n",
    "                aleatoria[busca] = random.choice(DATA_EMPRESAS).strip().split(' ')[busca]\n",
    "                break\n",
    "            except:\n",
    "                busca-=1\n",
    "    return ' '.join(aleatoria)\n",
    "\n",
    "def obtener_via_aleatoria():\n",
    "    return random.choice(DATA_VIAS).strip()\n",
    "\n",
    "def obtener_nif_aleatorio(texto = \"00000000A\"):\n",
    "    resultado = \"\"\n",
    "    for char in texto:\n",
    "        if char.isalpha():\n",
    "            resultado += random.choice(string.ascii_letters.upper())\n",
    "        elif char.isdigit():\n",
    "            resultado += random.choice(string.digits)\n",
    "        else:\n",
    "            resultado += char\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_persona_aleatoria(nombre_completo):\n",
    "    palabras = nombre_completo.split()\n",
    "    num_palabras = len(palabras)\n",
    "    \n",
    "    persona = nombre_completo \n",
    "    if num_palabras == 1: #Raro, pero bueno        \n",
    "        persona = obtener_nombre_aleatorio()\n",
    "    elif num_palabras == 2:\n",
    "        # Un nombre y un apellido\n",
    "        persona =  obtener_nombre_aleatorio() + ' ' + obtener_apellido_aleatorio()\n",
    "    elif num_palabras>2:\n",
    "        # Más de dos palabras: nombres y dos apellidos combinados\n",
    "        nombres_a_usar = num_palabras - 2\n",
    "        persona = ' '.join([obtener_nombre_aleatorio() for _ in range(nombres_a_usar)])\n",
    "        persona += ' ' + ' '.join([obtener_apellido_aleatorio() for _ in range(2)])            \n",
    "    return re_sub_case('.*',persona,nombre_completo,re.IGNORECASE+re.DOTALL)\n",
    "\n",
    "def obtener_fecha_aleatoria():\n",
    "    start_date = datetime.date(2020, 1, 1)\n",
    "    end_date = datetime.date(2023, 12, 31)\n",
    "    # Generar una fecha aleatoria entre start_date y end_date\n",
    "    tiempo_entre_fechas = end_date - start_date\n",
    "    dias_aleatorios = random.randrange(tiempo_entre_fechas.days)\n",
    "    fecha_aleatoria = start_date + datetime.timedelta(days=dias_aleatorios)\n",
    "    return convertir_fecha_a_texto_completo(fecha_aleatoria.strftime('%Y-%m-%d'))\n",
    "\n",
    "def obtener_protocolo_aleatorio():\n",
    "    return num2words(random.randrange(1,10000),lang='es')\n",
    "\n",
    "def obtener_texto_aleatorio(texto):   \n",
    "    \"\"\"Esta función cambia cada letra o número del texto por una letra o número aleatorios.\\n\n",
    "    No conserva ningún sentido, pero mantiene los saltos de página, los guiones y signos de puntuación.\"\"\" \n",
    "    resultado = \"\"\n",
    "    for char in texto:\n",
    "        if char.isalpha():\n",
    "            resultado += random.choice(string.ascii_letters.upper())\n",
    "        elif char.isdigit():\n",
    "            resultado += random.choice(string.digits)\n",
    "        else:\n",
    "            resultado += char\n",
    "    return re_sub_case('.*',resultado,texto,re.IGNORECASE+re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tipo_Diccionario(Enum):\n",
    "    NOMBRE  = auto()\n",
    "    APELLIDO = auto()\n",
    "    VIA = auto()\n",
    "    EMPRESA = auto()\n",
    "\n",
    "\"\"\"Definición para poder tener a mano los datos de cada fichero, buscando por tipo\"\"\"\n",
    "DICCIONARIO = {\n",
    "        Tipo_Diccionario.NOMBRE: DATA_NOMBRES, \n",
    "        Tipo_Diccionario.APELLIDO: DATA_APELLIDOS,\n",
    "        Tipo_Diccionario.EMPRESA: DATA_EMPRESAS,\n",
    "        Tipo_Diccionario.VIA: DATA_VIAS\n",
    "    }\n",
    "\n",
    "def buscar_en_diccionario(diccionario, texto):\n",
    "    \"\"\"Busca TODAS las ocurrencias de las palabras del diccionario en el texto\"\"\"\n",
    "    automaton = ahocorasick.Automaton()\n",
    "    for idx, key in enumerate([d.strip() for d in diccionario if len(d.strip())>3]):\n",
    "        # automaton.add_word(key.lower(), (idx, key))\n",
    "        automaton.add_word(key.lower(), key)\n",
    "\n",
    "    automaton.make_automaton()\n",
    "    resultado = []\n",
    "    for end_index, key in automaton.iter(texto.lower()):\n",
    "        # El algoritmo busca subcadenas, así que voy a usar regex sobre este subconjunto de coincidencias para buscar palabras enteras\n",
    "        matches = re.finditer(r'\\b'+key+r'\\b',texto,re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            ocurrencia = {\n",
    "                'start_index': match.start(),\n",
    "                'text': match.group()\n",
    "            }            \n",
    "            resultado.append(ocurrencia)\n",
    "    resultado.sort(key=lambda x: (x['start_index'], -len(x['text'])))\n",
    "    \n",
    "    # Quito los resultados que están contenidos en otros más grades\n",
    "    def esta_contenido(resultado, otros):\n",
    "        inicio = resultado['start_index']\n",
    "        fin = resultado['start_index'] + len(resultado['text'])                \n",
    "        for otro in otros:\n",
    "            otro_inicio = otro['start_index']\n",
    "            otro_fin = otro_inicio + len(otro['text'])\n",
    "            if inicio >= otro_inicio and fin <= otro_fin and resultado['text'] != otro['text']:            \n",
    "                return True                \n",
    "        return False\n",
    "    \n",
    "    resultados_filtrados = [r for r in resultado if not esta_contenido(r,resultado)]\n",
    "\n",
    "    return resultados_filtrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_aleatorio(tipo_diccionario, texto=None):\n",
    "    if tipo_diccionario == Tipo_Diccionario.NOMBRE:\n",
    "        return obtener_nombre_aleatorio()\n",
    "    elif tipo_diccionario == Tipo_Diccionario.APELLIDO:\n",
    "        return obtener_apellido_aleatorio()\n",
    "    elif tipo_diccionario == Tipo_Diccionario.EMPRESA:\n",
    "        return obtener_empresa_aleatoria(texto)\n",
    "    elif tipo_diccionario == Tipo_Diccionario.VIA:\n",
    "        return obtener_via_aleatoria()\n",
    "\n",
    "def anonimiza_con_diccionario(tipo_diccionario, texto):\n",
    "    partes_nuevo_texto = []\n",
    "    \n",
    "    coincidencias = buscar_en_diccionario(DICCIONARIO.get(tipo_diccionario),texto)\n",
    "    if len(coincidencias)==0:\n",
    "        return texto\n",
    "    # Si no hay coincidencias se devuelve el mismo texto, en otro caso, vamos sustituyendo\n",
    "    ultimo_index = 0\n",
    "    for coincidencia in coincidencias:\n",
    "        partes_nuevo_texto.append(texto[ultimo_index:coincidencia['start_index']])\n",
    "        partes_nuevo_texto.append(obtener_aleatorio(tipo_diccionario, coincidencia['text']))\n",
    "        ultimo_index = coincidencia['start_index'] + len(coincidencia['text']) \n",
    "    # Guardar el último\n",
    "    partes_nuevo_texto.append(texto[ultimo_index:])\n",
    "\n",
    "    return ''.join(partes_nuevo_texto)\n",
    "        \n",
    "\n",
    "def anonimiza(tipo_contenido:Tipo_Contenido, texto:str):\n",
    "    if texto == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    if tipo_contenido == Tipo_Contenido.PROTOCOLO:\n",
    "        return obtener_protocolo_aleatorio()\n",
    "    elif tipo_contenido == Tipo_Contenido.FECHA:\n",
    "        return obtener_fecha_aleatoria()\n",
    "    elif tipo_contenido == Tipo_Contenido.NOTARIO:\n",
    "        return obtener_persona_aleatoria(texto)\n",
    "    elif tipo_contenido == Tipo_Contenido.TIPO_DOCUMENTO:\n",
    "        return texto #No anonimizo el tipo\n",
    "    else:\n",
    "        # Anonimizar un contexto\n",
    "        #Buscar números\n",
    "        texto = re.sub(r'\\d+',lambda match:obtener_texto_aleatorio(match.group()),texto)\n",
    "        #Buscar correos electrónicos\n",
    "        texto = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}', lambda match:obtener_texto_aleatorio(match.group()), texto)\n",
    "        #Buscar web\n",
    "        texto = re.sub(r'[A-Za-z0-9._%+-:\\/\\/]+\\.(com|es|eu|org|net|info|biz|cat|edu|gov)\\b', lambda match:obtener_texto_aleatorio(match.group()), texto)        \n",
    "        #Buscar Empresas\n",
    "        texto = anonimiza_con_diccionario(Tipo_Diccionario.EMPRESA, texto)\n",
    "        #Buscar Personas (nombres / apellidos)\n",
    "        texto = anonimiza_con_diccionario(Tipo_Diccionario.APELLIDO, texto)\n",
    "        texto = anonimiza_con_diccionario(Tipo_Diccionario.NOMBRE, texto)    \n",
    "        #Buscar calles\n",
    "        texto = anonimiza_con_diccionario(Tipo_Diccionario.VIA, texto)\n",
    "        return texto\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código para generar el dataset anonimizando el contexto y respetando los espacios donde haya datos, que también serán anonimizados    \n",
    "def generar_datos_QA_anonimizado(archivo_xml, xml_precargado=None, texto_precargado=None):\n",
    "    escritura = generar_datos_QA(archivo_xml, usar_txt_normalizado=True,xml_precargado=xml_precargado,texto_precargado=texto_precargado)\n",
    "    # Localizar: crear una lista con datos=[{'answer_start', 'text', 'type'}] y ordenarla por 'answer_start'\n",
    "    datos = []\n",
    "    for qa in escritura['qas']:\n",
    "        for answer in qa['answers']:\n",
    "            tipo = Tipo_Contenido.tipo(qa['question'])\n",
    "            datos.append({\n",
    "                'answer_start': answer['answer_start'],\n",
    "                'text': answer['text'],\n",
    "                'type': tipo\n",
    "            })\n",
    "    # Ordenar 'datos' por 'answer_start'\n",
    "    datos.sort(key=lambda x: x['answer_start'])\n",
    "\n",
    "    # Separar: crear una lista partes=[{'type','text'}] que guarde de forma ordenada el 'context' fracturado según los datos y el espacio entre datos\n",
    "    partes = []\n",
    "    ultimo_fin = 0\n",
    "\n",
    "    for dato in datos:\n",
    "        # Añadir el texto del contexto anterior si es necesario\n",
    "        if dato['answer_start'] > ultimo_fin:\n",
    "            partes.append({\n",
    "                'type': Tipo_Contenido.CONTEXT,\n",
    "                'text': escritura['context'][ultimo_fin:dato['answer_start']]\n",
    "\n",
    "            })\n",
    "        # Añadir la parte de la respuesta\n",
    "        inicio_respuesta = dato['answer_start']\n",
    "        fin_respuesta = inicio_respuesta + len(dato['text'])\n",
    "        partes.append({\n",
    "            'type': dato['type'],\n",
    "            'text': escritura['context'][inicio_respuesta:fin_respuesta]\n",
    "        })\n",
    "        # Actualizar el último índice del final de la respuesta\n",
    "        ultimo_fin = fin_respuesta\n",
    "\n",
    "    # Añadir cualquier texto restante del contexto después de la última respuesta\n",
    "    if ultimo_fin < len(escritura['context']):\n",
    "        partes.append({\n",
    "            'type': Tipo_Contenido.CONTEXT,\n",
    "            'text': escritura['context'][ultimo_fin:]\n",
    "        })\n",
    "\n",
    "    # Anonimizar: para cada parte, según su 'tipo' realizar una función de anonimización\n",
    "    protocolo_anonimo = \"\"\n",
    "    fecha_anonimo = \"\"\n",
    "    notario_anonimo = \"\"\n",
    "    tipo_anonimo = \"\"\n",
    "    partes_anonimizadas = []\n",
    "    for p in partes:\n",
    "        tipo_p = p['type']\n",
    "        texto_p = p['text']\n",
    "        parte_anonimizada = {'start': sum(len(parte['text']) for parte in partes_anonimizadas),\n",
    "                             'type': tipo_p}\n",
    "        if tipo_p == Tipo_Contenido.PROTOCOLO:\n",
    "            if protocolo_anonimo == \"\":\n",
    "                protocolo_anonimo = anonimiza(tipo_p,texto_p)         \n",
    "            parte_anonimizada['text']=protocolo_anonimo            \n",
    "        elif tipo_p == Tipo_Contenido.FECHA:\n",
    "            if fecha_anonimo == \"\":\n",
    "                fecha_anonimo = anonimiza(tipo_p,texto_p)            \n",
    "            parte_anonimizada['text']=fecha_anonimo\n",
    "        elif tipo_p == Tipo_Contenido.NOTARIO:\n",
    "            if notario_anonimo == \"\":\n",
    "                notario_anonimo = anonimiza(tipo_p,texto_p)                        \n",
    "            parte_anonimizada['text']=notario_anonimo\n",
    "        elif tipo_p == Tipo_Contenido.TIPO_DOCUMENTO:\n",
    "            if tipo_anonimo == \"\":\n",
    "                tipo_anonimo = anonimiza(tipo_p,texto_p)                        \n",
    "            parte_anonimizada['text']=tipo_anonimo\n",
    "        else:\n",
    "            parte_anonimizada['text']=anonimiza(tipo_p,texto_p)\n",
    "        partes_anonimizadas.append(parte_anonimizada)\n",
    "        \n",
    "\n",
    "    # Juntar, de nuevo las partes, teniendo en cuenta que ahora los 'answer_start' y los 'text' han cambiado para los datos\n",
    "    texto_final = ''.join(p['text'] for p in partes_anonimizadas)\n",
    "\n",
    "    # Generamos un nuevo objeto escritura_anonimizada y componemos los datos.\n",
    "    escritura_anonimizada = {\n",
    "        'id_documento': escritura['id_documento'],\n",
    "        'context': texto_final,\n",
    "        'qas': []\n",
    "    }\n",
    "    # Hay que tener cuidado con los nuevos 'answer_start'\n",
    "    respuestas_por_pregunta = {\n",
    "        Tipo_Contenido.pregunta(Tipo_Contenido.PROTOCOLO): [],\n",
    "        Tipo_Contenido.pregunta(Tipo_Contenido.FECHA): [],\n",
    "        Tipo_Contenido.pregunta(Tipo_Contenido.NOTARIO): [],\n",
    "        Tipo_Contenido.pregunta(Tipo_Contenido.TIPO_DOCUMENTO): []\n",
    "    }\n",
    "    for p in partes_anonimizadas:\n",
    "        # Compruebo si esta parte corresponde con una pregunta\n",
    "        question = Tipo_Contenido.pregunta(p['type'])\n",
    "        if question is not None:\n",
    "            respuesta = {\n",
    "                'answer_start':p['start'],\n",
    "                'text': p['text']\n",
    "            }\n",
    "            respuestas_por_pregunta[question].append(respuesta)\n",
    "    \n",
    "    for pregunta, respuestas in respuestas_por_pregunta.items():\n",
    "        # Verificar si la lista de respuestas para esta pregunta está vacía\n",
    "        if not respuestas:\n",
    "            # Añadir la respuesta predeterminada\n",
    "            respuestas.append({'answer_start': -1, 'text': ''})\n",
    "        \n",
    "        # Añadir la pregunta y sus respuestas \n",
    "        escritura_anonimizada['qas'].append({\n",
    "            'question': pregunta,\n",
    "            'answers': respuestas\n",
    "        })\n",
    "\n",
    "    return escritura_anonimizada\n",
    "\n",
    "def generar_dataset_QA_anonimizado(directorio_origen, ruta_archivo_destino):\n",
    "    \"\"\"Genera un archivo .json con el dataset de los archivos de escritura ANONIMIZADOS.\\n\n",
    "    El fichero contendrá un objeto data con una lista de escrituras, cada escritura tiene los datos que produce la función 'gererar_datos_QA'.\\n\n",
    "    Después de pasar por todos los procesos de anonimización todos los nombres estarán cambiados, los números estarán cambiados y las direcciones y nombres de empresa estarán cambiados.\"\"\"\n",
    "    contador_ejecuciones = 0\n",
    "    try:\n",
    "        resultado = {'data':[]}\n",
    "        # Para iterar utilizamos las fichas (archivos xml), comprobando que exista su fichero normalizado.\n",
    "        lista_archivos = [os.path.join(directorio_origen, a) for a in os.listdir(directorio_origen) if a.endswith('.xml') and os.path.exists(os.path.join(directorio_origen, a.replace('.xml','_norm.txt')))]\n",
    "        xml_precargados = [ET.parse(ruta_archivo_xml) for ruta_archivo_xml in lista_archivos]        \n",
    "        textos_precargados = []\n",
    "        for ruta_archivo_xml in lista_archivos:\n",
    "            ruta_archivo_norm = ruta_archivo_xml.replace('.xml','_norm.txt')\n",
    "            with open(ruta_archivo_norm, 'r', encoding='utf-8') as txt_file:\n",
    "                textos_precargados.append(txt_file.read())\n",
    "            \n",
    "        print(\"DEBUG\", \"Iniciamos generación de datos anonimizados\", len(lista_archivos), \"Archivos por procesar.\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ejecutor:    \n",
    "            t_inicio = time.time()\n",
    "            # Ejecuciones guarda un objeto Future por cada archivo xml, con los resultados de su ejecución\n",
    "            ejecuciones = {ejecutor.submit(generar_datos_QA_anonimizado, archivo_xml,pre_xml,pre_txt):archivo_xml \n",
    "                           for archivo_xml,pre_xml,pre_txt in zip(lista_archivos,xml_precargados,textos_precargados)}\n",
    "            for ejecucion in concurrent.futures.as_completed(ejecuciones): # conforme se van completando van apareciendo                          \n",
    "                escritura = ejecucion.result()\n",
    "                if escritura is not None:\n",
    "                    resultado['data'].append({'escritura':escritura})\n",
    "                contador_ejecuciones += 1\n",
    "                if contador_ejecuciones % 500 == 0:\n",
    "                    print(\"DEBUG\", contador_ejecuciones, f\"Ejecuciones en {time.time()-t_inicio} segundos\")\n",
    "                    t_inicio = time.time()\n",
    "    except Exception as ex:\n",
    "        print(\"ERROR generando dataset QA anonimizado\\n\",ex)\n",
    "        traceback.print_exc()\n",
    "    finally:    \n",
    "        with open(ruta_archivo_destino, 'w', encoding='utf-8') as outfile:    \n",
    "            json.dump(resultado, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_dataset_QA_anonimizado(PDF_FICHAS_DIR,os.path.join(DATA_DIR,'dataset_QA_ANONIMIZADO.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspeccionar el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargo el dataset para trabajar en pandas. Lo inspeccionaré para ver que está todo correcto y generaré tres ficheros. Para entrenamiento, validación y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del archivo\n",
    "archivo_dataset_QA_anonimizado = os.path.join(DATA_DIR,'dataset_QA_ANONIMIZADO.json')\n",
    "with open(archivo_dataset_QA_anonimizado, 'r',encoding='utf-8') as archivo:\n",
    "    datos_json_QA_anonimizado = pd.read_json(archivo)\n",
    "\n",
    "# Hay que aplanar la estructura, para que cada fila sea una pregunta, con una respuesta, y mantenga el contexto y el id_documento del nivel superior.\n",
    "qa_dataset = pd.json_normalize(\n",
    "    data=datos_json_QA_anonimizado['data'],  \n",
    "    record_path=['escritura', 'qas'],  \n",
    "    meta=[['escritura', 'id_documento'], ['escritura', 'context']]  # Meta datos para preservar    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PRUEBAS\n",
    "# print(qa_dataset.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a ver cuántos tienen las tres respuestas encontradas\n",
    "def tiene_respuesta(respuestas):\n",
    "    # al menos, una respuesta válida\n",
    "    return any(respuesta['text'] != \"\" for respuesta in respuestas)\n",
    "\n",
    "def todo_el_grupo_tiene_respuestas(grupo):\n",
    "    # para cada elemento del grupo compruebo que tiene al menos una respuesta válida\n",
    "    # si todos son válidos, el grupo es válido\n",
    "    return all(tiene_respuesta(fila['answers']) for index, fila in grupo.iterrows())\n",
    "    \n",
    "\n",
    "qa_completo = qa_dataset.groupby('escritura.id_documento').filter(todo_el_grupo_tiene_respuestas)\n",
    "print(\"Número de escrituras con todos sus campos localizados:\", qa_completo['escritura.id_documento'].nunique())\n",
    "print(\"Número de documentos total:\", len(qa_dataset.groupby('escritura.id_documento')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a dejar guardados tres archivos, uno para entrenamiento, otro para validación y otro para test. \n",
    "\n",
    "En cada uno, tenemos en una única línea un objeto 'data' coneniendo una lista de 'escritura'. Cada escritura con sus campos 'id_documento', 'context' (el texto de la escritura) y una lista de preguntas-respuestas 'qas', que contiene objetos con 'question' (la pregunta para el modelo) y 'answers' que son listas de 'answer_start' y 'text' (las respuestas y su posición en el contexto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(datos_json_QA_anonimizado, test_size=0.2, random_state=42)\n",
    "train_data, vali_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "elementos = [('train', train_data), ('validation', vali_data), ('test', test_data)]\n",
    "for archivo, datos in elementos:\n",
    "    ruta_archivo = os.path.join(DATA_DIR, 'ESCRITURAS', f'QA_{archivo}.json')\n",
    "    with open(ruta_archivo,'w', encoding='utf-8') as f:\n",
    "        json.dump({'data':list(datos['data'])}, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar con estos ficheros de datos, en el directorio 'Dataset/ESCRITURAS' se creará una clase ESCRITURAS, que permitirá cargar los ficheros con las librerías dataset de Hugging Face."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
