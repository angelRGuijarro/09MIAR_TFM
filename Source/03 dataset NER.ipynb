{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrenamiento NER necesito el dataset con un formato diferente. Usaremos los mismos datos, y las preguntas nos servirán para identificar cada tipo de dato. Protocolos, Fechas, Nombres de notario y Tipos de documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from globals import DATA_DIR, TIPO_ETIQUETA, Tipo_Contenido, label2id, id2label\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos(archivo_json):\n",
    "    with open(archivo_json, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)['data']\n",
    "    \n",
    "def etiqueta_tipo(tipo:Tipo_Contenido):\n",
    "    \"\"\"Indica qué tipo de etiqueta es este tipo. Sólo sufijo de la etiqueta, pues no se conoce la posición.\"\"\"\n",
    "    try:\n",
    "        return TIPO_ETIQUETA[tipo]    \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def generar_ner_dataset(data):\n",
    "    dataset_ner = []\n",
    "    for item in data:\n",
    "        for paragraph in item['paragraphs']:\n",
    "            id = paragraph['id']\n",
    "            context = paragraph['context']\n",
    "            tokens = context.split()\n",
    "            labels = ['O'] * len(tokens) \n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                tipo = etiqueta_tipo(Tipo_Contenido.tipo(question))\n",
    "                for answer in qa['answers']:                    \n",
    "                    answer_start = answer['answer_start']\n",
    "                    answer_tokens = answer['text'].split()                    \n",
    "                    # Calcular índices de inicio y fin en tokens\n",
    "                    start_index = len(context[:answer_start].split())\n",
    "                    # es uno más, para que funcione el range\n",
    "                    end_index = start_index + len(answer_tokens)\n",
    "\n",
    "                    # Actualizar labels para esta respuesta\n",
    "                    for i in range(start_index, end_index):\n",
    "                        tag = 'B-' + tipo if i == start_index else 'I-' + tipo\n",
    "                        labels[i] = tag\n",
    "            ner_tags = [label2id[label] for label in labels]\n",
    "            dataset_ner.append({'id': id, 'tokens': tokens, 'ner_tags': ner_tags, 'labels': labels})\n",
    "    return dataset_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos y generar splits\n",
    "archivo_json = os.path.join(DATA_DIR, 'dataset_QA_ANONIMIZADO.json')\n",
    "datos_json = cargar_datos(archivo_json)\n",
    "ner_dataset = generar_ner_dataset(datos_json)\n",
    "train_val, test_data = train_test_split(ner_dataset, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_val, test_size=0.2, random_state=42)\n",
    "# Guardar datasets\n",
    "archivos = [('train', train_data), ('validation', val_data), ('test', test_data)]\n",
    "for nombre, data in archivos:\n",
    "    with open(os.path.join(DATA_DIR,'ESCRITURAS', f'NER_{nombre}.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(data,f,ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
