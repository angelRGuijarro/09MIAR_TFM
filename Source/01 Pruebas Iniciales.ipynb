{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS INICIALES\n",
    "Para ver la eficacia del modelo pre-entrenado, sin haber hecho ningún ajuste sobre los datos particulares. Registramos los resultados en MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from  metrics.evaluar_metricas import evaluar_metricas_QA, evaluar_metricas_NER\n",
    "import mlflow\n",
    "import pprint\n",
    "import requests\n",
    "from globals import DATA_DIR,MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizaremos las pruebas iniciales con el conjunto de test, cuando se haya concluido el entrenamiento del modelo ajustado (usando train y validation) volveremos a tomar la medida con este conjunto de test. Esto nos servirá para ver si efectivamente el modelo ajustado ha mejorado en algún modo el rendimiento del modelo pre-entrenado.\n",
    "# Datos y modelo\n",
    "dataset_QA = load_dataset(os.path.join(DATA_DIR, \"Escrituras\"),'QA',trust_remote_code=True,split='test')\n",
    "dataset_NER = load_dataset(os.path.join(DATA_DIR, \"Escrituras\"),'NER',trust_remote_code=True,split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_modelo_QA = os.path.join(MODELS_DIR,'PlanTL-GOB-ES','roberta-large-bne-sqac')\n",
    "ruta_modelo_NER = os.path.join(MODELS_DIR,'PlanTL-GOB-ES','roberta-base-bne-capitel-ner-plus')\n",
    "# Tamaño de lote para la inferenca\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVIDOR_MLFLOW = 'http://localhost:5000'\n",
    "# Debo comprobar si está ejecutando el servidor MLflow, en otro caso se demora la ejecución y acaba dando un error\n",
    "def mlflow_en_ejecucion(url):\n",
    "    try:\n",
    "        response = requests.get(url)        \n",
    "        # Si el servidor está en ejecución, deberíamos recibir un código de estado HTTP 200\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        # Si no se puede establecer una conexión, asumimos que el servidor no está en ejecución\n",
    "        return False\n",
    "    \n",
    "assert mlflow_en_ejecucion(SERVIDOR_MLFLOW), f\"El servidor MLflow ({SERVIDOR_MLFLOW}) no está en ejecución. Lance 'mlflow ui' desde el terminal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/18 13:13:36 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/02/18 13:13:36 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n"
     ]
    }
   ],
   "source": [
    "# Servidor de seguimiento\n",
    "mlflow.set_tracking_uri(SERVIDOR_MLFLOW)\n",
    "mlflow.autolog()\n",
    "mlflow.set_experiment(\"01 Valoración inicial\")\n",
    "# Impresión elegante de datos en la terminal\n",
    "pp = pprint.PrettyPrinter(width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='',\n",
      "            citation='',\n",
      "            homepage='',\n",
      "            license='',\n",
      "            features={'answers': Sequence(feature={'answer_start': Value(dtype='int32', id=None), 'text': Value(dtype='string', id=None)},\n",
      "                                          length=-1,\n",
      "                                          id=None),\n",
      "                      'context': Value(dtype='string', id=None),\n",
      "                      'id': Value(dtype='string', id=None),\n",
      "                      'question': Value(dtype='string', id=None)},\n",
      "            post_processed=None,\n",
      "            supervised_keys=None,\n",
      "            task_templates=None,\n",
      "            builder_name='escrituras',\n",
      "            dataset_name='escrituras',\n",
      "            config_name='QA',\n",
      "            version=0.0.0,\n",
      "            splits={'test': SplitInfo(name='test', num_bytes=11002385, num_examples=7532, shard_lengths=None, dataset_name='escrituras'),\n",
      "                    'train': SplitInfo(name='train', num_bytes=35013784, num_examples=24096, shard_lengths=None, dataset_name='escrituras'),\n",
      "                    'validation': SplitInfo(name='validation', num_bytes=8786051, num_examples=6028, shard_lengths=None, dataset_name='escrituras')},\n",
      "            download_checksums={'QA_test.json': {'checksum': None, 'num_bytes': 3590989},\n",
      "                                'QA_train.json': {'checksum': None, 'num_bytes': 11427604},\n",
      "                                'QA_validation.json': {'checksum': None, 'num_bytes': 2866185}},\n",
      "            download_size=17884778,\n",
      "            post_processing_size=None,\n",
      "            dataset_size=54802220,\n",
      "            size_in_bytes=72686998)\n"
     ]
    }
   ],
   "source": [
    "# Un vistazo a los datos que estamos testeando\n",
    "pp.pprint(dataset_QA.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='Dataset para entrenamiento de modelos NER en extracción de datos de escrituras.\\n'\n",
      "                        '                                    Las etiquetas utilizadas se corresponden con los siguientes elementos:\\n'\n",
      "                        \"                                    'B-PROTO','I-PROTO':    Número de PROTOCOLO.\\n\"\n",
      "                        \"                                    'B-FDOC','I-FDOC':      FECHA de firma del DOCUMENTO.\\n\"\n",
      "                        \"                                    'B-NOT','I-NOT':        NOTARIO, nombre y apellidos.\\n\"\n",
      "                        \"                                    'B-TDOC','I-TDOC':      TIPO de DOCUMENTO.\\n\"\n",
      "                        '                                ',\n",
      "            citation='',\n",
      "            homepage='',\n",
      "            license='',\n",
      "            features={'id': Value(dtype='string', id=None),\n",
      "                      'ner_tags': Sequence(feature=ClassLabel(names=['O',\n",
      "                                                                     'B-PROTO',\n",
      "                                                                     'I-PROTO',\n",
      "                                                                     'B-FDOC',\n",
      "                                                                     'I-FDOC',\n",
      "                                                                     'B-NOT',\n",
      "                                                                     'I-NOT',\n",
      "                                                                     'B-TDOC',\n",
      "                                                                     'I-TDOC'],\n",
      "                                                              id=None),\n",
      "                                           length=-1,\n",
      "                                           id=None),\n",
      "                      'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)},\n",
      "            post_processed=None,\n",
      "            supervised_keys=None,\n",
      "            task_templates=None,\n",
      "            builder_name='escrituras',\n",
      "            dataset_name='escrituras',\n",
      "            config_name='NER',\n",
      "            version=0.0.0,\n",
      "            splits={'test': SplitInfo(name='test', num_bytes=7462889, num_examples=1883, shard_lengths=None, dataset_name='escrituras'),\n",
      "                    'train': SplitInfo(name='train', num_bytes=23787339, num_examples=6024, shard_lengths=None, dataset_name='escrituras'),\n",
      "                    'validation': SplitInfo(name='validation', num_bytes=5962213, num_examples=1507, shard_lengths=None, dataset_name='escrituras')},\n",
      "            download_checksums={'NER_test.json': {'checksum': None, 'num_bytes': 7730085},\n",
      "                                'NER_train.json': {'checksum': None, 'num_bytes': 24632197},\n",
      "                                'NER_validation.json': {'checksum': None, 'num_bytes': 6173699}},\n",
      "            download_size=38535981,\n",
      "            post_processing_size=None,\n",
      "            dataset_size=37212441,\n",
      "            size_in_bytes=75748422)\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(dataset_NER.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Comprobación de si está bien instalado torch y tenemos una GPU\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También he visto que hay varias maneras de realizar las inferencias o consultas sobre el modelo, usando los pipelines. Quiero ver si hay alguna diferencia en el rendimiento. Probaré con distintas formas y cambiaré el tamaño del lote para ver si consigo un rendimiento óptimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline sobre las características del conjunto de datos\n",
    "Tras varias pruebas, no se observa una diferencia significativa en el rendimiento con distintos métodos de utilización de los pipelines, el más sencillo y que aprovecha la paralelización y trabajo en lotes, es pasar directamente las features de nuestro dataset al pipeline. \n",
    "\n",
    "Este es por tanto el méotodo finalmente implementado en evaluar_metricas_QA, que usaré tanto aquí como al final del entrenamiento QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Models\\PlanTL-GOB-ES\\roberta-large-bne-sqac\n",
      "\tf1: 0.41752380975914744\n",
      "\texact: 0.3413436006372809\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Prueba inicial de evaluación QA\",description=\"Predicciones con el modelo pre-entrenado, usando dataset de test\"):\n",
    "    evaluar_metricas_QA(ruta_modelo_QA,dataset_QA,split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at ..\\Models\\PlanTL-GOB-ES\\roberta-base-bne-capitel-ner-plus and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([17, 768]) in the checkpoint and torch.Size([9, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([17]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Models\\PlanTL-GOB-ES\\roberta-base-bne-capitel-ner-plus\n",
      "\tf1: 8.413967185527977e-05\n",
      "\tprecision: 4.408782294330306e-05\n",
      "\trecall: 0.0009191176470588235\n",
      "\taccuracy: 0.1829839129109202\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Prueba inicial de evaluación NER\",description=\"Predicciones con el modelo pre-entrenado, usando dataset de test\"):\n",
    "    evaluar_metricas_NER(ruta_modelo_NER,dataset_NER,split='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
