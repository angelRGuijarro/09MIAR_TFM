{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from metrics.evaluate import compute_exact, compute_f1\n",
    "from statistics import mean\n",
    "import mlflow\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, \\\n",
    "    pipeline, DataCollatorWithPadding\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import pprint\n",
    "# Impresión elegante de datos en la terminal\n",
    "pp = pprint.PrettyPrinter(width=150)\n",
    "from tqdm import tqdm\n",
    "from globals import TRAINING_DIR,MODELS_DIR, DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de variables globales, parámetros de entrenamiento y MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES GLOBALES\n",
    "train_max = None # Número máximo de elementos para entrenamiento (para pruebas) None para ir en serio\n",
    "training_output_dir = os.path.join(TRAINING_DIR,\"QA\")\n",
    "# Defino una serie de variables que registraré en los entrenamientos de MLflow\n",
    "ml_params = {\n",
    "    'num_epochs': 2,\n",
    "    'lr' : 1e-5,\n",
    "    'eval_steps' : 0.05, \n",
    "    'eval_batch_size' : 64,\n",
    "    'model_name': os.path.join(MODELS_DIR,'PlanTL-GOB-ES','roberta-large-bne-sqac')\n",
    "}\n",
    "num_epochs = lr = eval_steps = save_steps = eval_batch_size = model_name = 0\n",
    "for key, value in ml_params.items():\n",
    "    assert not globals()[key] is None, f'La variable global {key} debe estar definida'    \n",
    "    globals()[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga del conjunto de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset = load_dataset(os.path.join(DATA_DIR,'Escrituras'), 'QA', trust_remote_code=True)\n",
    "train_dataset = main_dataset['train']\n",
    "val_dataset = main_dataset['validation']\n",
    "if train_max:\n",
    "    train_dataset = train_dataset.select(range(train_max))\n",
    "    val_dataset = val_dataset.select(range(train_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobación de que el servidor MLflow está funcionando para las pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVIDOR_MLFLOW = 'http://localhost:5000'\n",
    "# Debo comprobar si está ejecutando el servidor MLflow, en otro caso se demora la ejecución y acaba dando un error\n",
    "def mlflow_en_ejecucion(url):\n",
    "    try:\n",
    "        response = requests.get(url)        \n",
    "        # Si el servidor está en ejecución, deberíamos recibir un código de estado HTTP 200\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        # Si no se puede establecer una conexión, asumimos que el servidor no está en ejecución\n",
    "        return False\n",
    "    \n",
    "assert mlflow_en_ejecucion(SERVIDOR_MLFLOW), f\"El servidor MLflow ({SERVIDOR_MLFLOW}) no está en ejecución. Lance 'mlflow ui' desde el terminal.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iniciamos el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/17 13:00:54 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n",
      "2024/02/17 13:00:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/423598931169215837', creation_time=1707602045463, experiment_id='423598931169215837', last_update_time=1707602045463, lifecycle_stage='active', name='ENTRENAMIENTO Question-Answering', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Servidor de seguimiento\n",
    "mlflow.set_tracking_uri(SERVIDOR_MLFLOW)\n",
    "mlflow.autolog()\n",
    "mlflow.set_experiment(\"ENTRENAMIENTO Question-Answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraído del tuturial en HF sobre Question-Answering\n",
    "def f_preproceso(examples):\n",
    "    \"\"\"Función para generar los input_ids, atention_mask y otras características para el entrenamiento\"\"\"\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    labels = []\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        # la secuencia indica qué tokens son de pregunta y cuales de contexto\n",
    "        sequence_ids = inputs.sequence_ids(i) \n",
    "\n",
    "        # Busca el inicio y el final del contexto\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # Si la pregunta no está íntegra en el contexto etiquetamos con (0,0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            labels.append([0,0])\n",
    "        else:\n",
    "            # En otro caso, se encuentra entre los tokens de inicio y final\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start = idx - 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end =idx + 1\n",
    "            end_positions.append(idx + 1)\n",
    "            labels.append([start,end])\n",
    "        \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    inputs[\"labels\"] = labels \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ffae1b7dfd41c69235e7d9370e3061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9b19039bfd487a9f8e0d61208862a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenized = train_dataset.map(f_preproceso,batched=True, remove_columns=train_dataset.column_names)\n",
    "eval_tokenized = val_dataset.map(f_preproceso,batched=True, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='',\n",
      "            citation='',\n",
      "            homepage='',\n",
      "            license='',\n",
      "            features={'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
      "                      'end_positions': Value(dtype='int64', id=None),\n",
      "                      'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
      "                      'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
      "                      'start_positions': Value(dtype='int64', id=None)},\n",
      "            post_processed=None,\n",
      "            supervised_keys=None,\n",
      "            task_templates=None,\n",
      "            builder_name='escrituras',\n",
      "            dataset_name='escrituras',\n",
      "            config_name='QA',\n",
      "            version=0.0.0,\n",
      "            splits={'test': SplitInfo(name='test', num_bytes=11002385, num_examples=7532, shard_lengths=None, dataset_name='escrituras'),\n",
      "                    'train': SplitInfo(name='train', num_bytes=35013784, num_examples=24096, shard_lengths=None, dataset_name='escrituras'),\n",
      "                    'validation': SplitInfo(name='validation', num_bytes=8786051, num_examples=6028, shard_lengths=None, dataset_name='escrituras')},\n",
      "            download_checksums={'QA_test.json': {'checksum': None, 'num_bytes': 3590989},\n",
      "                                'QA_train.json': {'checksum': None, 'num_bytes': 11427604},\n",
      "                                'QA_validation.json': {'checksum': None, 'num_bytes': 2866185}},\n",
      "            download_size=17884778,\n",
      "            post_processing_size=None,\n",
      "            dataset_size=54802220,\n",
      "            size_in_bytes=72686998)\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(train_tokenized.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = evaluate.load('squad_v2')\n",
    "     \n",
    "def compute_metrics(eval_pred):\n",
    "    pred_ini = np.argmax(eval_pred.predictions[0],axis=1)\n",
    "    pred_fin = np.argmax(eval_pred.predictions[1],axis=1)\n",
    "    pred_txt = [tokenizer.decode(tokens[p_ini:p_fin+1]).strip() for tokens,p_ini,p_fin in zip(eval_pred.inputs,pred_ini,pred_fin)]\n",
    "    \n",
    "    gold_ini = eval_pred.label_ids[0]\n",
    "    gold_fin = eval_pred.label_ids[1]\n",
    "    gold_txt = [tokenizer.decode(tokens[g_ini:g_fin+1]).strip() for tokens,g_ini,g_fin in zip(eval_pred.inputs,gold_ini,gold_fin)]\n",
    "    \n",
    "    f1s = [compute_f1(g,p) for g,p in zip(gold_txt,pred_txt)]\n",
    "    ems = [compute_exact(g,p) for g,p in zip(gold_txt,pred_txt)]\n",
    "\n",
    "    # return metric.compute(predictions=eval_pred.predictions, references=eval_pred.label_ids)\n",
    "    return {'f1_score':np.mean(f1s), 'exact_score': np.mean(ems)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arg = TrainingArguments(\n",
    "    output_dir=training_output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    learning_rate=lr,\n",
    "    warmup_ratio=0.2,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy='steps',\n",
    "    save_steps=save_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_score',\n",
    "    logging_steps=eval_steps,\n",
    "    # per_device_train_batch_size=batch_size,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    include_inputs_for_metrics=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arg,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,    \n",
    "    tokenizer=tokenizer,    \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f\"{'Prueba con ' + str(train_max) if train_max else 'Entrenamiento'}\"):\n",
    "    trainer.train()    \n",
    "    batch_size = trainer._train_batch_size\n",
    "    mlflow.log_param('batch_size',batch_size)\n",
    "    for param_name, param_value in ml_params.items():\n",
    "        mlflow.log_param(param_name, param_value)    \n",
    "    # Guardar el modelo\n",
    "    n_epochs = trainer.args.num_train_epochs\n",
    "    g_steps = trainer.state.global_step\n",
    "    fecha_hora = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    ruta_modelo_ajustado = os.path.join(MODELS_DIR,f\"{fecha_hora}_escrituras_QA_{n_epochs}-epoch_{g_steps}-steps\")\n",
    "    mlflow.log_param('finetuned_name', os.path.basename(ruta_modelo_ajustado))\n",
    "    trainer.save_model(ruta_modelo_ajustado)\n",
    "    tokenizer.save_pretrained(ruta_modelo_ajustado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.992911696434021,\n",
       " 'start': 81,\n",
       " 'end': 103,\n",
       " 'answer': 'Paquito de los Palotes'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_pipeline = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=trainer.model,\n",
    "    # batch_size=64,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "comprobando = ({'question': \"¿qué notario ha firmado el documento?\", 'context': \"DOS MIL TREINTA. En mi residencia, a quince de abril de dos mil quince. Ante mí, Paquito de los Palotes, notario del ilustre colegio de la Palmilla COMPARECEN Manolito y Jacinta para firmar la siguiente escritura de HERENCIA y para lo cual se sientan cómodamente.\"})\n",
    "\n",
    "print(tuned_pipeline(comprobando))\n",
    "comprobando = ({'question': \"¿cuál es el número de protocolo?\", 'context': \"DOS MIL TREINTA. En mi residencia, a quince de abril de dos mil quince. Ante mí, Paquito de los Palotes, notario del ilustre colegio de la Palmilla COMPARECEN Manolito y Jacinta para firmar la siguiente escritura de HERENCIA y para lo cual se sientan cómodamente.\"})\n",
    "\n",
    "print(tuned_pipeline(comprobando))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Models/20240213-2010_escrituras_QA_2-epoch_6024-steps\\\\tokenizer_config.json',\n",
       " '../Models/20240213-2010_escrituras_QA_2-epoch_6024-steps\\\\special_tokens_map.json',\n",
       " '../Models/20240213-2010_escrituras_QA_2-epoch_6024-steps\\\\vocab.json',\n",
       " '../Models/20240213-2010_escrituras_QA_2-epoch_6024-steps\\\\merges.txt',\n",
       " '../Models/20240213-2010_escrituras_QA_2-epoch_6024-steps\\\\added_tokens.json',\n",
       " '../Models/20240213-2010_escrituras_QA_2-epoch_6024-steps\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar el modelo\n",
    "n_epochs = trainer.args.num_train_epochs\n",
    "g_steps = trainer.state.global_step\n",
    "fecha_hora = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "ruta_modelo_ajustado = os.path.join(MODELS_DIR,f\"{fecha_hora}_escrituras_QA_{n_epochs}-epoch_{g_steps}-steps\")\n",
    "trainer.save_model(ruta_modelo_ajustado)\n",
    "tokenizer.save_pretrained(ruta_modelo_ajustado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo generado\n",
    "previo a la evaluación hay que hacer un Restart del entorno en VS Code... el sistema se me queda sin memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/15 16:13:23 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n",
      "2024/02/15 16:13:23 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      " 14%|█▍        | 1/7 [04:37<27:43, 277.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Models/20240213-1643_escrituras_QA_1-epoch_3012-steps\n",
      "\tf1: 0.8830989511427486\n",
      "\texact: 0.873473181093999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [09:13<23:02, 276.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Models/20240213-2010_escrituras_QA_2-epoch_6024-steps\n",
      "\tf1: 0.8838459227071735\n",
      "\texact: 0.8749336165693044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [13:49<18:25, 276.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Models/20240214-0110_escrituras_QA_5-epoch_5271-steps\n",
      "\tf1: 0.8780158036477616\n",
      "\texact: 0.8671003717472119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [18:24<13:46, 275.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Models/20240214-0746_escrituras_QA_5-epoch_4518-steps\n",
      "\tf1: 0.8814044241018563\n",
      "\texact: 0.8720127456186936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [22:58<09:10, 275.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Models/20240214-1608_escrituras_QA_5-epoch_4518-steps\n",
      "\tf1: 0.8814044241018563\n",
      "\texact: 0.8720127456186936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [27:32<00:00, 236.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Models/escrituras_QA_1-epoch_750-steps\n",
      "\tf1: 0.880648175345845\n",
      "\texact: 0.8720127456186936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(SERVIDOR_MLFLOW)\n",
    "mlflow.autolog()\n",
    "mlflow.set_experiment(\"ENTRENAMIENTO Question-Answering\")\n",
    "\n",
    "split_test = 'test'\n",
    "test_dataset = main_dataset[split_test] # load_dataset('../Dataset/Escrituras','QA',trust_remote_code=True,split='validation')\n",
    "for ruta in tqdm([r for r in os.listdir('../Models/') if os.path.isdir('../Models/'+r)]):\n",
    "    if \"escrituras_QA\" in ruta:\n",
    "        ruta_modelo_ajustado = f\"../Models/{ruta}\"\n",
    "\n",
    "        modelo_ajustado = AutoModelForQuestionAnswering.from_pretrained(ruta_modelo_ajustado)\n",
    "        tokenizer_ajustado = AutoTokenizer.from_pretrained(ruta_modelo_ajustado)\n",
    "        val_batch_size = 64\n",
    "\n",
    "        with mlflow.start_run(run_name=\"VALIDACIÓN\",description=\"Validación del modelo ajustado\"):\n",
    "            # Definición del pipeline y el conjunto de datos\n",
    "            qc_dataset_test = [{'question':q, 'context':c} for q,c in zip(test_dataset['question'],test_dataset['context'])]\n",
    "            consulta_qc = pipeline(\"question-answering\", model=modelo_ajustado, tokenizer=tokenizer_ajustado, \n",
    "                            device=0 if torch.cuda.is_available() else None, batch_size=val_batch_size)\n",
    "            # Ejecución y cálculo de métricas\n",
    "            predicciones = consulta_qc(qc_dataset_test)\n",
    "            # exact_scores, f1_scores  = get_raw_scores(test_dataset,predicciones)\n",
    "            gold_answers = [answer['text'][0] for answer in test_dataset['answers']]\n",
    "            pred_answers = [pred['answer'] for pred in predicciones]\n",
    "            f1_scores = [compute_f1(g,p) for g,p in zip(gold_answers,pred_answers)]\n",
    "            exact_scores = [compute_exact(g,p) for g,p in zip(gold_answers,pred_answers)]\n",
    "            \n",
    "            f1_mean = mean(f1_scores)\n",
    "            exact_mean = mean(exact_scores)\n",
    "            \n",
    "            for param_name, param_value in ml_params.items():\n",
    "                mlflow.log_param(param_name, param_value)        \n",
    "            mlflow.log_param('split', split_test)\n",
    "            mlflow.log_param('finetuned_name', os.path.basename(modelo_ajustado.name_or_path))\n",
    "            mlflow.log_metric('f1', f1_mean)\n",
    "            mlflow.log_metric('exact', exact_mean)\n",
    "            print(modelo_ajustado.name_or_path)\n",
    "            print('\\tf1:', f1_mean)\n",
    "            print('\\texact:', exact_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
