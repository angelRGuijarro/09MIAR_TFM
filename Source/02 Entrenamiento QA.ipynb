{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from custom_evaluate import get_raw_scores_by_prediction, compute_exact, compute_f1\n",
    "import evaluate\n",
    "from statistics import mean\n",
    "import mlflow\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, \\\n",
    "    pipeline, DataCollatorWithPadding\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "import datetime\n",
    "import pprint\n",
    "# Impresión elegante de datos en la terminal\n",
    "pp = pprint.PrettyPrinter(width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES GLOBALES\n",
    "train_max = None # Número máximo de elementos para entrenamiento (para pruebas) None para ir en serio\n",
    "training_output_dir = \"../training/QA\"\n",
    "ml_params = {\n",
    "    'num_epochs': 2,\n",
    "    # 'batch_size': 8, \n",
    "    'lr' : 1e-5,\n",
    "    'eval_steps' : 0.05, \n",
    "    'save_steps' : 0.05, \n",
    "    'eval_batch_size' : 128,\n",
    "    'model_name': 'PlanTL-GOB-ES/roberta-large-bne-sqac' \n",
    "}\n",
    "num_epochs = lr = eval_steps = save_steps = eval_batch_size = model_name = 0\n",
    "for key, value in ml_params.items():\n",
    "    assert not globals()[key] is None, f'La variable global {key} debe estar definida'    \n",
    "    globals()[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVIDOR_MLFLOW = 'http://localhost:5000'\n",
    "# Debo comprobar si está ejecutando el servidor MLflow, en otro caso se demora la ejecución y acaba dando un error\n",
    "def mlflow_en_ejecucion(url):\n",
    "    try:\n",
    "        response = requests.get(url)        \n",
    "        # Si el servidor está en ejecución, deberíamos recibir un código de estado HTTP 200\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        # Si no se puede establecer una conexión, asumimos que el servidor no está en ejecución\n",
    "        return False\n",
    "    \n",
    "assert mlflow_en_ejecucion(SERVIDOR_MLFLOW), f\"El servidor MLflow ({SERVIDOR_MLFLOW}) no está en ejecución. Lance 'mlflow ui' desde el terminal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Servidor de seguimiento\n",
    "mlflow.set_tracking_uri(SERVIDOR_MLFLOW)\n",
    "mlflow.autolog()\n",
    "mlflow.set_experiment(\"ENTRENAMIENTO Question-Answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset = load_dataset('../Dataset/Escrituras', 'QA', trust_remote_code=True)\n",
    "train_dataset = main_dataset['train']\n",
    "val_dataset = main_dataset['validation']\n",
    "if train_max:\n",
    "    train_dataset = train_dataset.select(range(train_max))\n",
    "    val_dataset = val_dataset.select(range(train_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraído del tuturial en HF sobre Question-Answering\n",
    "def f_preproceso(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    labels = []\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        # la secuencia indica qué tokens son de pregunta y cuales de contexto\n",
    "        sequence_ids = inputs.sequence_ids(i) \n",
    "\n",
    "        # Busca el inicio y el final del contexto\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # Si la pregunta no está íntegra en el contexto etiquetamos con (0,0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            labels.append([0,0])\n",
    "        else:\n",
    "            # En otro caso, se encuentra entre los tokens de inicio y final\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start = idx - 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end =idx + 1\n",
    "            end_positions.append(idx + 1)\n",
    "            labels.append([start,end])\n",
    "        \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    inputs[\"labels\"] = labels \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train_dataset.map(f_preproceso,batched=True, remove_columns=train_dataset.column_names)\n",
    "eval_tokenized = val_dataset.map(f_preproceso,batched=True, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(train_tokenized.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# metric = evaluate.load('squad_v2')\n",
    "     \n",
    "def compute_metrics(eval_pred):\n",
    "    pred_ini = np.argmax(eval_pred.predictions[0],axis=1)\n",
    "    pred_fin = np.argmax(eval_pred.predictions[1],axis=1)\n",
    "    gold_ini = eval_pred.label_ids[0]\n",
    "    gold_fin = eval_pred.label_ids[1]\n",
    "    pred_txt = [tokenizer.decode(tokens[p_ini:p_fin+1]).strip() for tokens,p_ini,p_fin in zip(eval_pred.inputs,pred_ini,pred_fin)]\n",
    "    gold_txt = [tokenizer.decode(tokens[g_ini:g_fin+1]).strip() for tokens,g_ini,g_fin in zip(eval_pred.inputs,gold_ini,gold_fin)]\n",
    "    \n",
    "    f1s = [compute_f1(g,p) for g,p in zip(gold_txt,pred_txt)]\n",
    "    ems = [compute_exact(g,p) for g,p in zip(gold_txt,pred_txt)]\n",
    "\n",
    "    # return metric.compute(predictions=eval_pred.predictions, references=eval_pred.label_ids)\n",
    "    return {'f1_score':np.mean(f1s), 'exact_score': np.mean(ems)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrar el directorio de entrenamiento si existe\n",
    "# if os.path.exists(training_output_dir):\n",
    "#     shutil.rmtree(training_output_dir)\n",
    "\n",
    "training_arg = TrainingArguments(\n",
    "    output_dir=training_output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    learning_rate=lr,\n",
    "    warmup_ratio=0.2,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy='steps',\n",
    "    save_steps=save_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_score',\n",
    "    logging_steps=eval_steps,\n",
    "    # per_device_train_batch_size=batch_size,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    include_inputs_for_metrics=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arg,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,    \n",
    "    tokenizer=tokenizer,    \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f\"{'Prueba con ' + str(train_max) if train_max else 'Entrenamiento'}\"):\n",
    "    trainer.train()    \n",
    "    batch_size = trainer._train_batch_size\n",
    "    for param_name, param_value in ml_params.items():\n",
    "        mlflow.log_param(param_name, param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_pipeline = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=trainer.model,\n",
    "    # batch_size=64,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "comprobando = ({'question': \"¿qué notario ha firmado el documento?\", 'context': \"DOS MIL TREINTA. En mi residencia, a quince de abril de dos mil quince. Ante mí, Paquito de los Palotes, notario del ilustre colegio de la Palmilla COMPARECEN Manolito y Jacinta para firmar la siguiente escritura de HERENCIA y para lo cual se sientan cómodamente.\"})\n",
    "\n",
    "tuned_pipeline(comprobando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "n_epochs = trainer.args.num_train_epochs\n",
    "g_steps = trainer.state.global_step\n",
    "fecha_hora = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "ruta_modelo_ajustado = f\"../Models/{fecha_hora}_escrituras_QA_{n_epochs}-epoch_{g_steps}-steps\"\n",
    "trainer.save_model(ruta_modelo_ajustado)\n",
    "tokenizer.save_pretrained(ruta_modelo_ajustado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo generado\n",
    "previo a la evaluación hay que hacer un Restart del entorno en VS Code... el sistema se me queda sin memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(SERVIDOR_MLFLOW)\n",
    "mlflow.autolog()\n",
    "mlflow.set_experiment(\"ENTRENAMIENTO Question-Answering\")\n",
    "\n",
    "split_test = 'validation'\n",
    "test_dataset = main_dataset[split_test] # load_dataset('../Dataset/Escrituras','QA',trust_remote_code=True,split='validation')\n",
    "ruta_modelo_ajustado = \"../Models/20240213-1643_escrituras_QA_1-epoch_3012-steps\"\n",
    "modelo_ajustado = AutoModelForQuestionAnswering.from_pretrained(ruta_modelo_ajustado)\n",
    "tokenizer_ajustado = AutoTokenizer.from_pretrained(ruta_modelo_ajustado)\n",
    "val_batch_size = 512\n",
    "\n",
    "with mlflow.start_run(run_name=\"VALIDACIÓN\",description=\"Validación del modelo ajustado\"):\n",
    "    # Definición del pipeline y el conjunto de datos\n",
    "    qc_dataset_test = [{'question':q, 'context':c} for q,c in zip(test_dataset['question'],test_dataset['context'])]\n",
    "    consulta_qc = pipeline(\"question-answering\", model=modelo_ajustado, tokenizer=tokenizer_ajustado, \n",
    "                    device=0 if torch.cuda.is_available() else None, batch_size=val_batch_size)\n",
    "    # Ejecución y cálculo de métricas\n",
    "    predicciones = consulta_qc(qc_dataset_test)\n",
    "    exact_scores, f1_scores  = get_raw_scores_by_prediction(test_dataset,predicciones)\n",
    "    f1_mean = mean(f1_scores.values())\n",
    "    exact_mean = mean(exact_scores.values())\n",
    "    \n",
    "    for param_name, param_value in ml_params.items():\n",
    "        mlflow.log_param(param_name, param_value)\n",
    "    mlflow.log_param('split', split_test)\n",
    "    mlflow.log_metric('f1', f1_mean)\n",
    "    mlflow.log_metric('exact', exact_mean)\n",
    "    print(len(f1_scores), 'f1:', f1_mean)\n",
    "    print(len(exact_scores), 'exact:', exact_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
